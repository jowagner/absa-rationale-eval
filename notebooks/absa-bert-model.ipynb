{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2dda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking code from\n",
    "# https://github.com/jowagner/CA4023-NLP/blob/main/notebooks/sentiment-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba763b",
   "metadata": {},
   "source": [
    "## BERT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1491d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10\n",
      "Accumulating gradients of 4 batches\n"
     ]
    }
   ],
   "source": [
    "model_size          = 'base'  # choose between 'tiny', 'base' and 'large'\n",
    "max_sequence_length = 256\n",
    "batch_size          = 10\n",
    "\n",
    "# compensate for small batch size with batch accumulation if needed\n",
    "accumulate_grad_batches = 1\n",
    "while batch_size * accumulate_grad_batches < 32:\n",
    "    # accumulated batch size too small\n",
    "    # --> accumulate more batches\n",
    "    accumulate_grad_batches += 1\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "if accumulate_grad_batches > 1:\n",
    "    print('Accumulating gradients of %d batches' %accumulate_grad_batches)\n",
    "    \n",
    "size2name = {\n",
    "    'tiny':  'distilbert-base-uncased',\n",
    "    'base':  'bert-base-uncased',\n",
    "    'large': 'bert-large-uncased',\n",
    "}\n",
    "\n",
    "model_name = size2name[model_size]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25048744",
   "metadata": {},
   "source": [
    "## Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050dedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data/ABSA16_Laptops_Train_SB1_v2.xml\n"
     ]
    }
   ],
   "source": [
    "domain = 'laptop' # 'restaurant'\n",
    "\n",
    "data_prefix = 'data/'\n",
    "\n",
    "filenames = {\n",
    "    'laptop':     'ABSA16_Laptops_Train_SB1_v2.xml',\n",
    "    'restaurant': 'ABSA16_Restaurants_Train_SB1_v2.xml',\n",
    "}\n",
    "\n",
    "filename = data_prefix + filenames[domain]\n",
    "\n",
    "print('Using', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78784a7",
   "metadata": {},
   "source": [
    "## Get Data Instances from XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849419f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed entity types: ['BATTERY', 'COMPANY', 'CPU', 'DISPLAY', 'FANS_COOLING', 'GRAPHICS', 'HARDWARE', 'HARD_DISC', 'KEYBOARD', 'LAPTOP', 'MEMORY', 'MOTHERBOARD', 'MOUSE', 'MULTIMEDIA_DEVICES', 'OPTICAL_DRIVES', 'OS', 'PORTS', 'POWER_SUPPLY', 'SHIPPING', 'SOFTWARE', 'SUPPORT', 'WARRANTY']\n",
      "observed attribute labels: ['CONNECTIVITY', 'DESIGN_FEATURES', 'GENERAL', 'MISCELLANEOUS', 'OPERATION_PERFORMANCE', 'PORTABILITY', 'PRICE', 'QUALITY', 'USABILITY']\n",
      "observed polarities: ['negative', 'neutral', 'positive']\n",
      "number of unique targets: 0\n"
     ]
    }
   ],
   "source": [
    "# mostly implemented from scratch, some inspiration from\n",
    "# https://opengogs.adaptcentre.ie/rszk/sea/src/master/lib/semeval_absa.py\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "xmltree = ElementTree.parse(filename)\n",
    "xmlroot = xmltree.getroot()\n",
    "\n",
    "observed_entity_types = set()\n",
    "observed_attribute_labels = set()\n",
    "observed_polarities = set()\n",
    "observed_targets = set()\n",
    "\n",
    "dataset = []\n",
    "for sentence in xmlroot.iter('sentence'):\n",
    "    sent_id = sentence.get('id')\n",
    "    # get content inside the first <text>...</text> sub-element\n",
    "    text = sentence.findtext('text').strip()\n",
    "    #print(sent_id, text)\n",
    "    for opinion in sentence.iter('Opinion'):\n",
    "        opin_cat = opinion.get('category')\n",
    "        entity_type, attribute_label = opin_cat.split('#')\n",
    "        polarity = opinion.get('polarity')\n",
    "        target = opinion.get('target')\n",
    "        try:\n",
    "            span = (int(opinion.get('from')), int(opinion.get('to')))\n",
    "        except TypeError:\n",
    "            # at least one of 'from' or 'to' is missing\n",
    "            span = (0, 0)\n",
    "        if target == 'NULL':\n",
    "            target = None\n",
    "        # add to dataset\n",
    "        dataset.append((\n",
    "            sent_id, text,\n",
    "            entity_type, attribute_label,\n",
    "            target, span,\n",
    "            polarity\n",
    "        ))\n",
    "        # update vocabularies\n",
    "        observed_entity_types.add(entity_type)\n",
    "        observed_attribute_labels.add(attribute_label)\n",
    "        observed_polarities.add(polarity)\n",
    "        if target:\n",
    "            observed_targets.add(target)\n",
    "    \n",
    "print('observed entity types:',     sorted(observed_entity_types))\n",
    "print('observed attribute labels:', sorted(observed_attribute_labels))\n",
    "print('observed polarities:',       sorted(observed_polarities))\n",
    "print('number of unique targets:',  len(observed_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f85d9",
   "metadata": {},
   "source": [
    "## Example BERT Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1a3a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tinput:         This computer is absolutely AMAZING!!!\n",
      "\t['input_ids']: [101, 2023, 3274, 2003, 7078, 6429, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'this', 'computer', 'is', 'absolutely', 'amazing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "1 \tinput:         and plenty of storage with 250 gb(though I will upgrade this and the ram..)\n",
      "\t['input_ids']: [101, 1998, 7564, 1997, 5527, 2007, 5539, 16351, 1006, 2295, 1045, 2097, 12200, 2023, 1998, 1996, 8223, 1012, 1012, 1007, 102]\n",
      "\ttokens:        ['[CLS]', 'and', 'plenty', 'of', 'storage', 'with', '250', 'gb', '(', 'though', 'i', 'will', 'upgrade', 'this', 'and', 'the', 'ram', '.', '.', ')', '[SEP]']\n",
      "\n",
      "2 \tinput:         GET THIS COMPUTER FOR PORTABILITY AND FAST PROCESSING!!!\n",
      "\t['input_ids']: [101, 2131, 2023, 3274, 2005, 3417, 8010, 1998, 3435, 6364, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'get', 'this', 'computer', 'for', 'port', '##ability', 'and', 'fast', 'processing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "3 \tinput:         without a big ol' clunky machine in my backpack, I feel like I can do programming homework anywhere.\n",
      "\t['input_ids']: [101, 2302, 1037, 2502, 19330, 1005, 18856, 16814, 2100, 3698, 1999, 2026, 13383, 1010, 1045, 2514, 2066, 1045, 2064, 2079, 4730, 19453, 5973, 1012, 102]\n",
      "\ttokens:        ['[CLS]', 'without', 'a', 'big', 'ol', \"'\", 'cl', '##unk', '##y', 'machine', 'in', 'my', 'backpack', ',', 'i', 'feel', 'like', 'i', 'can', 'do', 'programming', 'homework', 'anywhere', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_batch = []\n",
    "for i in (0, 4, 8, 18):  # select a few interesting instances\n",
    "    example_batch.append(dataset[i][1])   \n",
    "\n",
    "tokenised_text = tokeniser(\n",
    "    example_batch,\n",
    "    is_split_into_words = False,\n",
    ")\n",
    "\n",
    "for i, token_ids in enumerate(tokenised_text['input_ids']):\n",
    "    if i: print()\n",
    "    print(i, '\\tinput:        ', example_batch[i])\n",
    "    print(   \"\\t['input_ids']:\", token_ids)\n",
    "    print(   '\\ttokens:       ', tokeniser.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01347f",
   "metadata": {},
   "source": [
    "## Sequence Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52b55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LengthBin\t    negative\t     neutral\t    positive\t       Total\t  Positivity\n",
      "   0-   9\t         123\t          20\t         279\t         422\t        66%\n",
      "  10-  19\t         484\t          75\t         744\t        1303\t        57%\n",
      "  20-  29\t         303\t          59\t         428\t         790\t        54%\n",
      "  30-  39\t         102\t          17\t         125\t         244\t        51%\n",
      "  40-  49\t          47\t          15\t          38\t         100\t        38%\n",
      "  50-  59\t          11\t           2\t          20\t          33\t        61%\n",
      "  60-  69\t           8\t           0\t           0\t           8\t         0%\n",
      "  70-  79\t           3\t           0\t           3\t           6\t        50%\n",
      "  80-  89\t           3\t           0\t           0\t           3\t         0%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "bin_width = 10\n",
    "\n",
    "distribution = defaultdict(lambda: 0)\n",
    "\n",
    "dataset.append(7*[None])\n",
    "batch = []\n",
    "labels = []\n",
    "max_length_bin = 0\n",
    "for _, text, _, _, _, _, label in dataset:\n",
    "    if text is not None:\n",
    "        batch.append(text)\n",
    "        labels.append(label)\n",
    "    if len(batch) == batch_size \\\n",
    "    or (text is None and len(batch) > 0):\n",
    "        tokenised_batch = tokeniser(\n",
    "            batch,\n",
    "            is_split_into_words = False,\n",
    "        )\n",
    "        for index, token_ids in enumerate(tokenised_batch['input_ids']):\n",
    "            label = labels[index]\n",
    "            length = len(token_ids)\n",
    "            length_bin = length // bin_width\n",
    "            distribution[(label,   length_bin)] += 1\n",
    "            distribution[('total', length_bin)] += 1\n",
    "            if length_bin > max_length_bin:\n",
    "                max_length_bin = length_bin\n",
    "        batch = []\n",
    "        labels = []\n",
    "                \n",
    "header = []\n",
    "header.append('LengthBin')\n",
    "for polarity in sorted(observed_polarities):\n",
    "    header.append('%12s' %polarity)\n",
    "header.append('%12s' %'Total')\n",
    "header.append('%12s' %'Positivity')\n",
    "print('\\t'.join(header))\n",
    "for length_bin in range(0, max_length_bin+1):\n",
    "    row = []\n",
    "    row.append('%4d-%4d' %(\n",
    "        bin_width*length_bin,\n",
    "        bin_width*(1+length_bin)-1\n",
    "    ))\n",
    "    total = 0\n",
    "    for label in sorted(observed_polarities):\n",
    "        count = distribution[(label, length_bin)]\n",
    "        row.append('%12d' %count)\n",
    "        total += count\n",
    "    row.append('%12d' %total)\n",
    "    if total:\n",
    "        row.append('%10.0f%%' %(100.0*distribution[('positive', length_bin)]/float(total)))\n",
    "    else:\n",
    "        row.append('%11s' %'n/a')\n",
    "    print('\\t'.join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5b5d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
