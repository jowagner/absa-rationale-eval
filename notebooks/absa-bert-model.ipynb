{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2dda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking code from\n",
    "# https://github.com/jowagner/CA4023-NLP/blob/main/notebooks/sentiment-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba763b",
   "metadata": {},
   "source": [
    "## 1.1 BERT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1491d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 10\n",
      "Accumulating gradients of 4 batches\n"
     ]
    }
   ],
   "source": [
    "model_size          = 'base'  # choose between 'tiny', 'base' and 'large'\n",
    "max_sequence_length = 256\n",
    "batch_size          = 10\n",
    "\n",
    "# compensate for small batch size with batch accumulation if needed\n",
    "accumulate_grad_batches = 1\n",
    "while batch_size * accumulate_grad_batches < 32:\n",
    "    # accumulated batch size too small\n",
    "    # --> accumulate more batches\n",
    "    accumulate_grad_batches += 1\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "if accumulate_grad_batches > 1:\n",
    "    print('Accumulating gradients of %d batches' %accumulate_grad_batches)\n",
    "    \n",
    "size2name = {\n",
    "    'tiny':  'distilbert-base-uncased',\n",
    "    'base':  'bert-base-uncased',\n",
    "    'large': 'bert-large-uncased',\n",
    "}\n",
    "\n",
    "model_name = size2name[model_size]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25048744",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050dedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data/ABSA16_Laptops_Train_SB1_v2.xml\n",
      "Using data/ABSA16_Restaurants_Train_SB1_v2.xml\n"
     ]
    }
   ],
   "source": [
    "domains = ['laptop', 'restaurant']\n",
    "\n",
    "train_dev_split = (90, 10)\n",
    "\n",
    "data_prefix = 'data/'\n",
    "\n",
    "filenames = {\n",
    "    'laptop':     'ABSA16_Laptops_Train_SB1_v2.xml',\n",
    "    'restaurant': 'ABSA16_Restaurants_Train_SB1_v2.xml',\n",
    "}\n",
    "\n",
    "for domain in domains:\n",
    "    filename = data_prefix + filenames[domain]\n",
    "    print('Using', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966290b",
   "metadata": {},
   "source": [
    "## 1.3 Question Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860c51dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "put_question_first = True  # whether to put question into seq A or B\n",
    "\n",
    "templates = [\n",
    "    \n",
    "    # Hoang et al. (2019)\n",
    "    {   'question': '%(entity_type)s, %(attribute_label)s',\n",
    "        'label':    '%(polarity)s',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 1\n",
    "    {   'question': '%(entity_type)s - %(attribute_label)s',\n",
    "        'label':    '%(polarity)s',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 2\n",
    "    {    'question': 'What do you think of the %(attribute_label)s of %(entity_type)s?',\n",
    "         'label':    '%(polarity)s',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 3\n",
    "    {    'question': 'The polarity of the aspect %(attribute_label)s of %(entity_type)s is %(candidate_polarity)s.',\n",
    "         'label':    '%(yesno)s',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 4\n",
    "    {   'question': '%(entity_type)s - %(attribute_label)s - %(candidate_polarity)s',\n",
    "        'label':    '%(yesno)s',\n",
    "    },\n",
    "    \n",
    "    # Variant 1\n",
    "    {    'question': 'In terms of %(attribute_label)s, what do you think of %(entity_type)s?',\n",
    "         'label':    '%(polarity)s',\n",
    "    },\n",
    "    \n",
    "    # Variant 2\n",
    "    {    'question': 'What polarity has the sentiment towards the %(attribute_label)s of %(entity_type)s in the following rewview?',\n",
    "         'label':    '%(polarity)s',\n",
    "    },\n",
    "    \n",
    "    # Variant 3\n",
    "    {    'question': 'Do you agree that the sentiment towards the aspect %(attribute_label)s of %(entity_type)s in the following review is %(candidate_polarity)s?',\n",
    "         'label':    '%(yesno)s',\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# TODO: add variants with entity type and attribute label not in ALLCAPS and\n",
    "#       with _ between words (requires additional code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78784a7",
   "metadata": {},
   "source": [
    "## 2.1 Get Data Instances from XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "849419f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed entity types: ['AMBIENCE', 'BATTERY', 'COMPANY', 'CPU', 'DISPLAY', 'DRINKS', 'FANS_COOLING', 'FOOD', 'GRAPHICS', 'HARDWARE', 'HARD_DISC', 'KEYBOARD', 'LAPTOP', 'LOCATION', 'MEMORY', 'MOTHERBOARD', 'MOUSE', 'MULTIMEDIA_DEVICES', 'OPTICAL_DRIVES', 'OS', 'PORTS', 'POWER_SUPPLY', 'RESTAURANT', 'SERVICE', 'SHIPPING', 'SOFTWARE', 'SUPPORT', 'WARRANTY']\n",
      "\n",
      "observed attribute labels: ['CONNECTIVITY', 'DESIGN_FEATURES', 'GENERAL', 'MISCELLANEOUS', 'OPERATION_PERFORMANCE', 'PORTABILITY', 'PRICE', 'PRICES', 'QUALITY', 'STYLE_OPTIONS', 'USABILITY']\n",
      "\n",
      "observed polarities: ['negative', 'neutral', 'positive']\n",
      "\n",
      "number of unique targets: 721\n"
     ]
    }
   ],
   "source": [
    "# mostly implemented from scratch, some inspiration from\n",
    "# https://opengogs.adaptcentre.ie/rszk/sea/src/master/lib/semeval_absa.py\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "observed_entity_types = set()\n",
    "observed_attribute_labels = set()\n",
    "observed_polarities = set()\n",
    "observed_targets = set()\n",
    "\n",
    "def get_dataset(filename):\n",
    "    global observed_entity_types\n",
    "    global observed_attribute_labels\n",
    "    global observed_polarities\n",
    "    global observed_targets\n",
    "    xmltree = ElementTree.parse(filename)\n",
    "    xmlroot = xmltree.getroot()\n",
    "    dataset = []\n",
    "    for sentence in xmlroot.iter('sentence'):\n",
    "        sent_id = sentence.get('id')\n",
    "        # get content inside the first <text>...</text> sub-element\n",
    "        text = sentence.findtext('text').strip()\n",
    "        #print(sent_id, text)\n",
    "        for opinion in sentence.iter('Opinion'):\n",
    "            opin_cat = opinion.get('category')\n",
    "            entity_type, attribute_label = opin_cat.split('#')\n",
    "            polarity = opinion.get('polarity')\n",
    "            target = opinion.get('target')\n",
    "            try:\n",
    "                span = (int(opinion.get('from')), int(opinion.get('to')))\n",
    "            except TypeError:\n",
    "                # at least one of 'from' or 'to' is missing\n",
    "                span = (0, 0)\n",
    "            if target == 'NULL':\n",
    "                target = None\n",
    "            # add to dataset\n",
    "            dataset.append((\n",
    "                sent_id, text,\n",
    "                entity_type, attribute_label,\n",
    "                target, span,\n",
    "                polarity\n",
    "            ))\n",
    "            # update vocabularies\n",
    "            observed_entity_types.add(entity_type)\n",
    "            observed_attribute_labels.add(attribute_label)\n",
    "            observed_polarities.add(polarity)\n",
    "            if target:\n",
    "                observed_targets.add(target)\n",
    "    return dataset\n",
    "\n",
    "datasets = []\n",
    "for domain in domains:\n",
    "    filename = data_prefix + filenames[domain]\n",
    "    datasets.append((domain, get_dataset(filename)))\n",
    "    \n",
    "print('observed entity types:',     sorted(observed_entity_types))\n",
    "print('\\nobserved attribute labels:', sorted(observed_attribute_labels))\n",
    "print('\\nobserved polarities:',       sorted(observed_polarities))\n",
    "print('\\nnumber of unique targets:',  len(observed_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f083d49",
   "metadata": {},
   "source": [
    "## 2.2 PyTorch DataLoader\n",
    "\n",
    "To use the PyTorch Lighting framwork, we need to distinguish 3 types of objects handling our data:\n",
    "\n",
    "### Dataset\n",
    "\n",
    "PyTorch Dataset objects provide access to a data set and behave like a list of dictionaries, one dictionary for each data instance (training or test item). The framework does not prescribe what the dictionaries look like, i.e. you can choose the keys. The length of the list determines the number of training instances in each epoch, unless the DataLoader (below) is extended to filter or augment the data. The standard way to augment data is to keep the number of instances identical to the number of raw instances and to apply a different or random transformation in each call of `__getitem__()`.\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "PyTorch DataLoader objects shuffle data provided by a Dataset object and create batches of data.\n",
    "\n",
    "### LightningDataModule\n",
    "\n",
    "LightningDataModule objects create 3 DataLoader objects, one each for training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic usage of pytorch and lightning from\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# and\n",
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "class ABSA_Dataset_part_1(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_data,\n",
    "        put_question_first = True,\n",
    "        question_prefix = None,\n",
    "        template_index = -1,    # -1 = pick random template\n",
    "        info = None,            # additional info to keep with each instance\n",
    "    ):\n",
    "        self.raw_data            = raw_data\n",
    "        self.put_question_first  = put_question_first\n",
    "        self.question_prefix     = question_prefix\n",
    "        self.template_index      = template_index\n",
    "        self.info                = info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' get one instance of the dataset as a custom dictionary\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            assert isinstance(idx, int)\n",
    "        sent_id, text, \\\n",
    "            entity_type, attribute_label, \\\n",
    "            target, span, \\\n",
    "            polarity = self.raw_data[idx]\n",
    "        question, label = self.pick_question(entity_type, attribute_label, polarity)\n",
    "        if self.question_prefix:\n",
    "            question = self.question_prefix + ' ' + question\n",
    "        # TODO: support adding context (previous sentences) to text\n",
    "        retval = {}\n",
    "        if self.put_question_first:\n",
    "            retval['seq_A'] = question\n",
    "            retval['seq_B'] = text\n",
    "        else:\n",
    "            retval['seq_A'] = text\n",
    "            retval['seq_B'] = question\n",
    "        retval['label'] = label\n",
    "        retval['info']  = self.info\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f674226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ABSA_Dataset(ABSA_Dataset_part_1):\n",
    "                   \n",
    "    def pick_question(self, entity_type, attribute_label, polarity):\n",
    "        global templates\n",
    "        global observed_polarities\n",
    "        if self.template_index < 0:\n",
    "            template = random.choice(templates)\n",
    "        else:\n",
    "            template = templates[self.template_index]\n",
    "        candidate_polarity = random.choice(list(observed_polarities))\n",
    "        if candidate_polarity == polarity:\n",
    "            yesno = 'yes'\n",
    "        else:\n",
    "            yesno = 'no'\n",
    "        question = template['question'] %locals()\n",
    "        label    = template['label']    %locals()\n",
    "        return (question, label)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c3a62",
   "metadata": {},
   "source": [
    "## 2.3 Training-Dev Split\n",
    "The SemEval ABSA dataset comes without a dev set. We need a dev set to decide how long to train, to select other parameters and to select a good run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ca122d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop 2909\n",
      "restaurant 2507\n",
      "Total size: 5416\n"
     ]
    }
   ],
   "source": [
    "# concatenate domains\n",
    "\n",
    "tr_dataset_objects = []\n",
    "\n",
    "for domain, dataset in datasets:\n",
    "    print(domain, len(dataset))\n",
    "    tr_dataset_objects.append(ABSA_Dataset(\n",
    "        dataset,\n",
    "        put_question_first = put_question_first,\n",
    "        question_prefix = domain + ':',\n",
    "        template_index  = 0,   # a template that keeps the original 3-value polarity\n",
    "        info = domain\n",
    "    ))\n",
    "\n",
    "tr_dataset = torch.utils.data.ConcatDataset(tr_dataset_objects)\n",
    "n = len(tr_dataset)\n",
    "print('Total size:', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c11369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('positive', 'laptop'): split 1473 (90.0%) to 164 (10.0%)\n",
      "('negative', 'laptop'): split 975 (89.9%) to 109 (10.1%)\n",
      "('neutral', 'laptop'): split 169 (89.9%) to 19 (10.1%)\n",
      "('negative', 'restaurant'): split 674 (90.0%) to 75 (10.0%)\n",
      "('positive', 'restaurant'): split 1491 (90.0%) to 166 (10.0%)\n",
      "('neutral', 'restaurant'): split 90 (89.1%) to 11 (10.9%)\n",
      "\n",
      "Training data size: 4872\n",
      "Development data size: 544\n"
     ]
    }
   ],
   "source": [
    "# how many instances are there for each label?\n",
    "\n",
    "group2indices = {}\n",
    "for index in range(n):\n",
    "    label = tr_dataset[index]['label']\n",
    "    domain = tr_dataset[index]['info']\n",
    "    group = (label, domain)\n",
    "    if not group in group2indices:\n",
    "        group2indices[group] = []\n",
    "    group2indices[group].append(index)\n",
    "\n",
    "# create stratified sample\n",
    "    \n",
    "rel_train_size, rel_dev_size = train_dev_split\n",
    "rel_total = rel_train_size + rel_dev_size\n",
    "\n",
    "tr_indices = []\n",
    "dev_indices = []\n",
    "\n",
    "for group in group2indices:\n",
    "    indices = group2indices[group]\n",
    "    n = len(indices)\n",
    "    select = (n * rel_train_size) // rel_total\n",
    "    remaining = n - select\n",
    "    print('%r: split %d (%.1f%%) to %d (%.1f%%)' %(\n",
    "        group, select, 100.0*select/float(n),\n",
    "        remaining, 100.0*remaining/float(n),\n",
    "    ))\n",
    "    random.shuffle(indices)\n",
    "    tr_indices += indices[:select]\n",
    "    dev_indices += indices[select:]\n",
    "\n",
    "tr_indices.sort()\n",
    "dev_indices.sort()\n",
    "\n",
    "dev_dataset = torch.utils.data.Subset(tr_dataset, dev_indices)\n",
    "tr_dataset  = torch.utils.data.Subset(tr_dataset, tr_indices)\n",
    "\n",
    "print()\n",
    "print('Training data size:', len(tr_dataset))\n",
    "print('Development data size:', len(dev_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193f8b",
   "metadata": {},
   "source": [
    "## 2.4 Lightning Wrapper for Training, Development and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "800185d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwagner/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/metrics/__init__.py:44: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  \"`pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package\"\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "class ABSA_DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, classifier, data_split = None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.hparams = classifier.hparams\n",
    "        self.classifier = classifier\n",
    "        if data_split is None:      # this happens when loading a checkpoint\n",
    "            data_split = (None, None, None)\n",
    "        self.data_split = data_split\n",
    "        self.kwargs = kwargs\n",
    "        self.label_encoder = LabelEncoder(\n",
    "            sorted(list(observed_polarities)) + ['yes', 'no'],\n",
    "            reserved_labels = [],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the training data '''\n",
    "        dataset = self.data_split[0]\n",
    "        return DataLoader(\n",
    "            dataset     = dataset,\n",
    "            sampler     = RandomSampler(dataset),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the validation data '''\n",
    "        return DataLoader(\n",
    "            dataset     = self.data_split[1],\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the test data '''\n",
    "        return DataLoader(\n",
    "            dataset     = self.data_split[2],\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e00ce",
   "metadata": {},
   "source": [
    "## 3.1 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a9e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_part_1(pl.LightningModule):\n",
    "    \n",
    "    #def __init__(self, hparams = None, **kwargs) -> None:\n",
    "    def __init__(self, hparams = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict:\n",
    "            #print('Converting', type(hparams))\n",
    "            hparams = pl.utilities.AttributeDict(hparams)\n",
    "        #print('New classifier with', hparams)\n",
    "        # https://discuss.pytorch.org/t/pytorch-lightning-module-cant-set-attribute-error/121125\n",
    "        self.hparams.update(hparams)\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.data = ABSA_DataModule(self, **kwargs)\n",
    "        if 'tokeniser' in kwargs:\n",
    "            self.tokenizer = kwargs['tokeniser']  # attribute expected by lightning\n",
    "        else:\n",
    "            # this happens when loading a checkpoint\n",
    "            self.tokenizer = None  # TODO: this may break ability to use the model\n",
    "        self.__build_model()\n",
    "        self.__build_loss()\n",
    "        # prepare training with frozen BERT layers so that the new\n",
    "        # classifier head can first adjust to BERT before BERT\n",
    "        # adjusts to the classifier in later epochs        \n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "        self.record_predictions = False\n",
    "            \n",
    "    def __build_model(self) -> None:\n",
    "        ''' Init BERT model, tokeniser and classification head '''\n",
    "        # Q: Why not use AutoModelForSequenceClassification?\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name,  # was: self.hparams.encoder_model\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        # parameters for the classification head: best values\n",
    "        # depend on the task and dataset; the below values\n",
    "        # have not been tuned much but work reasonable well\n",
    "        # for the P&L04 data\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1536),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1536, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, self.data.label_encoder.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __build_loss(self):\n",
    "        self._loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8780c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "\n",
    "class Classifier_part_2(Classifier_part_1):\n",
    "    \n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        if self._frozen:\n",
    "            log.info('\\n== Encoder model fine-tuning ==')\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "            \n",
    "    def freeze_encoder(self) -> None:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        ''' make a prediction for a single data instance '''\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_inputs, _ = self.prepare_sample(\n",
    "                [sample],\n",
    "                prepare_target = False\n",
    "            )\n",
    "            model_out = self.forward(batch_inputs)\n",
    "            logits = torch.Tensor.cpu(model_out[\"logits\"]).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits, axis=1)\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "        return sample\n",
    "    \n",
    "    # functionality to obtain predictions for a dataset as a\n",
    "    # side effect of asking PyTorch Lightning to get evaluation\n",
    "    # results for a dataset\n",
    "    # (the framework does not seem to provide a function to get\n",
    "    # all predictions for a dataset)\n",
    "    \n",
    "    def start_recording_predictions(self):\n",
    "        self.record_predictions = True\n",
    "        self.reset_recorded_predictions()\n",
    "        \n",
    "    def stop_recording_predictions(self):\n",
    "        self.record_predictions = False\n",
    "        \n",
    "    def reset_recorded_predictions(self):\n",
    "        self.seq2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.utils import lengths_to_mask\n",
    "\n",
    "class Classifier_part_3(Classifier_part_2):\n",
    "    \n",
    "    def forward(self, batch_input):\n",
    "        tokens  = batch_input['input_ids']\n",
    "        lengths = batch_input['length']\n",
    "        mask = batch_input['attention_mask']\n",
    "        # Run BERT model.\n",
    "        word_embeddings = self.bert(tokens, mask).last_hidden_state\n",
    "        sentemb = word_embeddings[:,0]  # at position of [CLS]\n",
    "        logits = self.classification_head(sentemb)\n",
    "        # Hack to conveniently use the model and trainer to\n",
    "        # get predictions for a test set:\n",
    "        if self.record_predictions:\n",
    "            logits_np = torch.Tensor.cpu(logits).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits_np, axis=1)\n",
    "            ]\n",
    "            for index, input_token_ids in enumerate(tokens):\n",
    "                key = torch.Tensor.cpu(input_token_ids).numpy().tolist()\n",
    "                # truncate trailing zeros\n",
    "                while key and key[-1] == 0:\n",
    "                    del key[-1]\n",
    "                self.seq2label[tuple(key)] = predicted_labels[index]\n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "    def loss(self, predictions: dict, targets: dict) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: model specific output. Must contain a key 'logits' with\n",
    "            a tensor [batch_size x 1] with model predictions\n",
    "        :param labels: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions[\"logits\"], targets[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a510a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Classifier_part_4(Classifier_part_3):\n",
    "    \n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\" prepare a batch of instances to pass them into the model\n",
    "        \n",
    "        :param sample: list of dictionaries.\n",
    "        \n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        assert len(sample) <= batch_size\n",
    "        assert self.tokenizer is not None\n",
    "        batch_seq_A = []\n",
    "        batch_seq_B = []\n",
    "        for item in sample:\n",
    "            batch_seq_A.append(item['seq_A'])\n",
    "            batch_seq_B.append(item['seq_B'])\n",
    "        # run the tokeniser\n",
    "        encoded_batch = self.tokenizer(\n",
    "            batch_seq_A,\n",
    "            batch_seq_B,\n",
    "            is_split_into_words = False,\n",
    "            return_length       = True,\n",
    "            padding             = 'max_length',\n",
    "            # https://github.com/huggingface/transformers/issues/8691\n",
    "            return_tensors      = 'pt',\n",
    "        )\n",
    "        if not prepare_target:\n",
    "            return encoded_batch, {}  # no target labels requested\n",
    "        # Prepare target:\n",
    "        batch_labels = []\n",
    "        for item in sample:\n",
    "            batch_labels.append(item['label'])\n",
    "        assert len(batch_labels) <= batch_size\n",
    "        try:\n",
    "            targets = {\n",
    "                \"labels\": self.data.label_encoder.batch_encode(batch_labels)\n",
    "            }\n",
    "            return encoded_batch, targets\n",
    "        except RuntimeError:\n",
    "            raise Exception(\"Label encoder found an unknown label.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caf5059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Classifier_part_5(Classifier_part_4):\n",
    "    \n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        ''' perform a training step with the given batch '''\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        # Q: What is this about?\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "        output = OrderedDict({\"loss\": loss_val})\n",
    "        self.log('train_loss', loss_val, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "   \n",
    "    def test_or_validation_step(self, test_type, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        ''' perform a test or validation step with the given batch '''\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        y = targets[\"labels\"]\n",
    "        # get predictions\n",
    "        y_hat = model_out[\"logits\"]\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        # get accuracy\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "        output = OrderedDict({\n",
    "            test_type + \"_loss\": loss_val,\n",
    "            test_type + \"_acc\":  val_acc,\n",
    "            'batch_size': len(batch),\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'val', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "    \n",
    "    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'test', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1e8cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class Classifier(Classifier_part_5):\n",
    "    \n",
    "    # validation_end() is now validation_epoch_end()\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/blob/efd272a3cac2c412dd4a7aa138feafb2c114326f/CHANGELOG.md\n",
    "    \n",
    "    def test_or_validation_epoch_end(self, test_type, outputs: list) -> None:\n",
    "        ''' calculate average loss and accuracy over all batches,\n",
    "            reducing the weight of the last batch according to its\n",
    "            size so that all data instances have equal influence\n",
    "            on the scores\n",
    "        '''\n",
    "        val_loss_mean = 0.0\n",
    "        val_acc_mean = 0.0\n",
    "        total_size = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output[test_type + \"_loss\"]\n",
    "            # reduce manually when using dp\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output[test_type + \"_acc\"]\n",
    "            if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "                val_acc = torch.mean(val_acc)\n",
    "            # We weight the batch accuracy by batch size to not give\n",
    "            # higher weight to the items of a smaller, final bacth.\n",
    "            batch_size = output['batch_size']\n",
    "            val_acc_mean += val_acc * batch_size\n",
    "            total_size += batch_size\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= total_size\n",
    "        self.log(test_type+'_loss', val_loss_mean)\n",
    "        self.log(test_type+'_acc',  val_acc_mean)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('val', outputs)\n",
    "                                     \n",
    "    def test_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('test', outputs)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.classification_head.parameters()},\n",
    "            {\n",
    "                \"params\": self.bert.parameters(),\n",
    "                \"lr\": self.hparams.encoder_learning_rate,\n",
    "                #\"weight_decay\": 0.01,  # TODO: try this as it is in the BERT paper\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434032b",
   "metadata": {},
   "source": [
    "## 4.1 Training\n",
    "\n",
    "TODO: move configuration to section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "113178f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = Classifier(\n",
    "    hparams = {\n",
    "        \"encoder_learning_rate\": 1e-05,  # Encoder specific learning rate\n",
    "        \"learning_rate\":         3e-05,  # Classification head learning rate\n",
    "        \"nr_frozen_epochs\":      3,      # Number of epochs we want to keep the encoder model frozen\n",
    "        \"loader_workers\":        4,      # How many subprocesses to use for data loading.\n",
    "                                         # (0 means that the data will be loaded in the main process)\n",
    "        \"batch_size\":            batch_size,\n",
    "        \"gpus\":                  1,\n",
    "    },\n",
    "    # parameters for SlicedDataModule:\n",
    "    data_split = (tr_dataset, dev_dataset),\n",
    "    # parameters for SlicedDocument():\n",
    "    tokeniser                   = tokeniser,\n",
    "    max_sequence_length         = max_sequence_length,\n",
    "    preproc_batch_size          = 8\n",
    ")   \n",
    "print('Ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4603b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | bert                | BertModel        | 109 M \n",
      "1 | classification_head | Sequential       | 1.6 M \n",
      "2 | _loss               | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "111 M     Total params\n",
      "444.233   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwagner/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:70: LightningDeprecationWarning: Internal: `use_dp` is deprecated in v1.2 and will be removed in v1.4.\n",
      "  rank_zero_deprecation(\"Internal: `use_dp` is deprecated in v1.2 and will be removed in v1.4.\")\n",
      "/home/jwagner/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/trainer/deprecated_api.py:92: LightningDeprecationWarning: Internal: `use_ddp2` is deprecated in v1.2 and will be removed in v1.4.\n",
      "  rank_zero_deprecation(\"Internal: `use_ddp2` is deprecated in v1.2 and will be removed in v1.4.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a61c5dce568403aaae4ba6552d44978",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 16 minutes\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import os\n",
    "import time\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor   = 'val_acc',\n",
    "    min_delta = 0.00,\n",
    "    patience  = 5,\n",
    "    verbose   = False,\n",
    "    mode      = 'max',\n",
    ")\n",
    "\n",
    "save_top_model_callback = ModelCheckpoint(\n",
    "    save_top_k = 3,\n",
    "    monitor    = 'val_acc',\n",
    "    mode       = 'max',\n",
    "    filename   = '{val_acc:.4f}-{epoch:02d}-{val_loss:.4f}'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stop_callback, save_top_model_callback],\n",
    "    max_epochs = 6,\n",
    "    min_epochs = classifier.hparams.nr_frozen_epochs + 2,\n",
    "    gpus = classifier.hparams.gpus,\n",
    "    accumulate_grad_batches = accumulate_grad_batches,   # compensate for small batch size\n",
    "    #limit_train_batches = 10,  # use only a subset of the data during development for higher speed\n",
    "    check_val_every_n_epoch = 1,\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "start = time.time()\n",
    "trainer.fit(classifier, classifier.data)\n",
    "print('Training time: %.0f minutes' %((time.time()-start)/60.0))\n",
    "\n",
    "## Appendix A: Example BERT Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d84bb2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2af72e5e4f6b98ce\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2af72e5e4f6b98ce\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=nCq_vy9qE-k at 44:59\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a77654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is /home/jwagner/Documents/research/interpretability/rationale-eval-2021/absa-rationale-eval/notebooks/lightning_logs/default/version_0/checkpoints/val_acc=0.8691-epoch=04-val_loss=0.4086.ckpt\n",
      "Best validation set accuracy: tensor(0.8691, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('The best model is', save_top_model_callback.best_model_path)\n",
    "\n",
    "print('Best validation set accuracy:', save_top_model_callback.best_model_score)\n",
    "\n",
    "# The following automatically loads the best weights according to\n",
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "\n",
    "# TODO: need test set\n",
    "#       + fix https://github.com/jowagner/absa-rationale-eval/issues/3 before proceeding\n",
    "#print('Test results via trainer.test():')\n",
    "#results = trainer.test()  # also prints results as a side effect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c6d8f",
   "metadata": {},
   "source": [
    "## 5.1 Save Best Model outside Logs\n",
    "\n",
    "Rather than manually locating the best model in the lightning logs folder and copying it to another location, use the library to save a copy. This also gives us the option to save a copy without the training state of the Adam optimiser, reducing model size by about 67%, training parameters and filesystem paths that we may not want to share with users of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77a6d7f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-70d9c0ac6162>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m best_model = Classifier.load_from_checkpoint(\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# the hparams including hparams.batch_size appear to have been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# saved in the checkpoint automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHECKPOINT_HYPER_PARAMS_KEY\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_model_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36m_load_model_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0m_cls_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls_init_args_name\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_cls_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# give model a chance to load something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-254da675d2b0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hparams, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print('New classifier with', hparams)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# https://discuss.pytorch.org/t/pytorch-lightning-module-cant-set-attribute-error/121125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mABSA_DataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html\n",
    "\n",
    "# after just having run test(), the best checkpoint is still loaded but that's\n",
    "# not a documented feature so to be on the safe side for future versions we\n",
    "# need to explicitly load the best checkpoint:\n",
    "\n",
    "best_model = Classifier.load_from_checkpoint(\n",
    "    checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "    # the hparams including hparams.batch_size appear to have been\n",
    "    # saved in the checkpoint automatically\n",
    ")\n",
    "# best_model.save_checkpoint('best.ckpt') does not exist\n",
    "# --> need to wrap model into trainer to be able to save a checkpoint\n",
    "\n",
    "new_trainer = pl.Trainer(\n",
    "    resume_from_checkpoint = trainer.checkpoint_callback.best_model_path,\n",
    "    gpus = -1,  # avoid warnings (-1 = automatic selection)\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "new_trainer.model = best_model  # @model.setter in plugins/training_type/training_type_plugin.py\n",
    "\n",
    "\n",
    "\n",
    "new_trainer.save_checkpoint(\n",
    "    \"best-model-weights-only.ckpt\",\n",
    "    True,  # save_weights_only\n",
    "    # (if saved with setting the 2nd arg to True, the checkpoint\n",
    "    # will contain absoulte paths and training parameters)\n",
    ")\n",
    "\n",
    "# to just save the bert model in pytorch format and without the classification head, we could follow\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3096#issuecomment-686877242\n",
    "best_model.bert.save_pretrained('best-bert-encoder.pt')\n",
    "\n",
    "# Since the lightning module inherits from pytorch, we can save the full network in\n",
    "# pytorch format:\n",
    "torch.save(best_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f85d9",
   "metadata": {},
   "source": [
    "## Appendix A: Example BERT Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1a3a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tinput:         This computer is absolutely AMAZING!!!\n",
      "\t['input_ids']: [101, 2023, 3274, 2003, 7078, 6429, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'this', 'computer', 'is', 'absolutely', 'amazing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "1 \tinput:         and plenty of storage with 250 gb(though I will upgrade this and the ram..)\n",
      "\t['input_ids']: [101, 1998, 7564, 1997, 5527, 2007, 5539, 16351, 1006, 2295, 1045, 2097, 12200, 2023, 1998, 1996, 8223, 1012, 1012, 1007, 102]\n",
      "\ttokens:        ['[CLS]', 'and', 'plenty', 'of', 'storage', 'with', '250', 'gb', '(', 'though', 'i', 'will', 'upgrade', 'this', 'and', 'the', 'ram', '.', '.', ')', '[SEP]']\n",
      "\n",
      "2 \tinput:         GET THIS COMPUTER FOR PORTABILITY AND FAST PROCESSING!!!\n",
      "\t['input_ids']: [101, 2131, 2023, 3274, 2005, 3417, 8010, 1998, 3435, 6364, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'get', 'this', 'computer', 'for', 'port', '##ability', 'and', 'fast', 'processing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "3 \tinput:         without a big ol' clunky machine in my backpack, I feel like I can do programming homework anywhere.\n",
      "\t['input_ids']: [101, 2302, 1037, 2502, 19330, 1005, 18856, 16814, 2100, 3698, 1999, 2026, 13383, 1010, 1045, 2514, 2066, 1045, 2064, 2079, 4730, 19453, 5973, 1012, 102]\n",
      "\ttokens:        ['[CLS]', 'without', 'a', 'big', 'ol', \"'\", 'cl', '##unk', '##y', 'machine', 'in', 'my', 'backpack', ',', 'i', 'feel', 'like', 'i', 'can', 'do', 'programming', 'homework', 'anywhere', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_batch = []\n",
    "for domain, dataset in datasets:\n",
    "    if domain == 'laptop':\n",
    "        for i in (0, 4, 8, 18):  # select a few interesting instances\n",
    "            example_batch.append(dataset[i][1])   \n",
    "\n",
    "tokenised_text = tokeniser(\n",
    "    example_batch,\n",
    "    is_split_into_words = False,\n",
    ")\n",
    "\n",
    "for i, token_ids in enumerate(tokenised_text['input_ids']):\n",
    "    if i: print()\n",
    "    print(i, '\\tinput:        ', example_batch[i])\n",
    "    print(   \"\\t['input_ids']:\", token_ids)\n",
    "    print(   '\\ttokens:       ', tokeniser.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01347f",
   "metadata": {},
   "source": [
    "## Appendix B: Sequence Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d52b55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "LengthBin\t    negative\t     neutral\t    positive\t       Total\t  Positivity\n",
      "   0-   9\t         123\t          20\t         279\t         422\t        66%\n",
      "  10-  19\t         484\t          75\t         744\t        1303\t        57%\n",
      "  20-  29\t         303\t          59\t         428\t         790\t        54%\n",
      "  30-  39\t         102\t          17\t         125\t         244\t        51%\n",
      "  40-  49\t          47\t          15\t          38\t         100\t        38%\n",
      "  50-  59\t          11\t           2\t          20\t          33\t        61%\n",
      "  60-  69\t           8\t           0\t           0\t           8\t         0%\n",
      "  70-  79\t           3\t           0\t           3\t           6\t        50%\n",
      "  80-  89\t           3\t           0\t           0\t           3\t         0%\n",
      "restaurant\n",
      "LengthBin\t    negative\t     neutral\t    positive\t       Total\t  Positivity\n",
      "   0-   9\t          74\t          13\t         276\t         363\t        76%\n",
      "  10-  19\t         300\t          52\t         724\t        1076\t        67%\n",
      "  20-  29\t         183\t          21\t         432\t         636\t        68%\n",
      "  30-  39\t         111\t          11\t         145\t         267\t        54%\n",
      "  40-  49\t          43\t           4\t          40\t          87\t        46%\n",
      "  50-  59\t          26\t           0\t          27\t          53\t        51%\n",
      "  60-  69\t           9\t           0\t           6\t          15\t        40%\n",
      "  70-  79\t           3\t           0\t           0\t           3\t         0%\n",
      "  80-  89\t           0\t           0\t           0\t           0\t        n/a\n",
      "  90-  99\t           0\t           0\t           7\t           7\t       100%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "bin_width = 10\n",
    "\n",
    "for domain, dataset in datasets:\n",
    "    print(domain)\n",
    "    distribution = defaultdict(lambda: 0)\n",
    "    dataset.append(7*[None])  # hack to simplify loop below\n",
    "    batch = []\n",
    "    labels = []\n",
    "    max_length_bin = 0\n",
    "    for _, text, _, _, _, _, label in dataset:\n",
    "        if text is not None:\n",
    "            batch.append(text)\n",
    "            labels.append(label)\n",
    "        if len(batch) == batch_size \\\n",
    "        or (text is None and len(batch) > 0):\n",
    "            tokenised_batch = tokeniser(\n",
    "                batch,\n",
    "                is_split_into_words = False,\n",
    "            )\n",
    "            for index, token_ids in enumerate(tokenised_batch['input_ids']):\n",
    "                label = labels[index]\n",
    "                length = len(token_ids)\n",
    "                length_bin = length // bin_width\n",
    "                distribution[(label,   length_bin)] += 1\n",
    "                distribution[('total', length_bin)] += 1\n",
    "                if length_bin > max_length_bin:\n",
    "                    max_length_bin = length_bin\n",
    "            batch = []\n",
    "            labels = []\n",
    "    del dataset[-1]  # remove \"end of data\" marker of hack above   \n",
    "    header = []\n",
    "    header.append('LengthBin')\n",
    "    for polarity in sorted(observed_polarities):\n",
    "        header.append('%12s' %polarity)\n",
    "    header.append('%12s' %'Total')\n",
    "    header.append('%12s' %'Positivity')\n",
    "    print('\\t'.join(header))\n",
    "    for length_bin in range(0, max_length_bin+1):\n",
    "        row = []\n",
    "        row.append('%4d-%4d' %(\n",
    "            bin_width*length_bin,\n",
    "            bin_width*(1+length_bin)-1\n",
    "        ))\n",
    "        total = 0\n",
    "        for label in sorted(observed_polarities):\n",
    "            count = distribution[(label, length_bin)]\n",
    "            row.append('%12d' %count)\n",
    "            total += count\n",
    "        row.append('%12d' %total)\n",
    "        if total:\n",
    "            row.append('%10.0f%%' %(100.0*distribution[('positive', length_bin)]/float(total)))\n",
    "        else:\n",
    "            row.append('%11s' %'n/a')\n",
    "        print('\\t'.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f75282",
   "metadata": {},
   "source": [
    "## Appendix C: Example Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba5b5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "0 {'seq_A': 'laptop: LAPTOP - GENERAL', 'seq_B': 'This computer is absolutely AMAZING!!!', 'label': 'positive', 'info': None}\n",
      "0 {'seq_A': 'laptop: LAPTOP - GENERAL - neutral', 'seq_B': 'This computer is absolutely AMAZING!!!', 'label': 'no', 'info': None}\n",
      "1 {'seq_A': 'laptop: What polarity has the sentiment towards the OPERATION_PERFORMANCE of BATTERY in the following rewview?', 'seq_B': '10 plus hours of battery...', 'label': 'positive', 'info': None}\n",
      "1 {'seq_A': 'laptop: BATTERY - OPERATION_PERFORMANCE', 'seq_B': '10 plus hours of battery...', 'label': 'positive', 'info': None}\n",
      "\n",
      "restaurant\n",
      "0 {'seq_A': 'restaurant: In terms of GENERAL, what do you think of RESTAURANT?', 'seq_B': 'Judging from previous posts this used to be a good place, but not any longer.', 'label': 'negative', 'info': None}\n",
      "0 {'seq_A': 'restaurant: RESTAURANT - GENERAL', 'seq_B': 'Judging from previous posts this used to be a good place, but not any longer.', 'label': 'negative', 'info': None}\n",
      "1 {'seq_A': 'restaurant: What polarity has the sentiment towards the GENERAL of SERVICE in the following rewview?', 'seq_B': 'We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.', 'label': 'negative', 'info': None}\n",
      "1 {'seq_A': 'restaurant: What polarity has the sentiment towards the GENERAL of SERVICE in the following rewview?', 'seq_B': 'We, there were four of us, arrived at noon - the place was empty - and the staff acted like we were imposing on them and they were very rude.', 'label': 'negative', 'info': None}\n"
     ]
    }
   ],
   "source": [
    "is_first = True\n",
    "for domain, dataset in datasets:\n",
    "    if not is_first: print()\n",
    "    print(domain)\n",
    "    dataset_obj = ABSA_Dataset(\n",
    "        dataset,\n",
    "        put_question_first = put_question_first,\n",
    "        question_prefix = domain + ':',\n",
    "        template_index = -1,   # -1 = random pick\n",
    "    )\n",
    "    for i in range(2):\n",
    "        print(i, dataset_obj[i])\n",
    "        print(i, dataset_obj[i])  # repeat call doesn't give the same result with template_index = -1\n",
    "    is_first = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2462c39",
   "metadata": {},
   "source": [
    "## Appendix D: Location of Instances Picked for Devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76188f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___d_____d__________d____d____d______________________dd__________dd__dd____\n",
      "_________d____________d____d______d________________dd__d_______d___d_______\n",
      "d___d_____d___dd________________dd____d__d___________________________d_____\n",
      "________________________d________________d___d_________________________d___\n",
      "______d___d______d__________d____________d___d_______d_______d__d__________\n",
      "_________________d___d___d__________________________________d__d_____d_____\n",
      "________________d_____d_____d___d____d____d____________________d___________\n",
      "_____________________dd__d______________d______d_______________d_________d_\n",
      "____d_______d______d_____________d_d_____d__d______d____________d________d_\n",
      "__________d____d_d_________________________d___dd__d______________________d\n"
     ]
    }
   ],
   "source": [
    "row = []\n",
    "n = len(tr_indices) + len(dev_indices)\n",
    "for i in range(n):\n",
    "    if i in dev_indices:\n",
    "        row.append('d')\n",
    "    else:\n",
    "        row.append('_')\n",
    "    if len(row) == 75 or i+1 == n:\n",
    "        print(''.join(row))\n",
    "        row = []\n",
    "    if i > 750:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b987f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f691b49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
