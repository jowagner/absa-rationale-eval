{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2dda7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking code from\n",
    "# https://github.com/jowagner/CA4023-NLP/blob/main/notebooks/sentiment-bert.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba763b",
   "metadata": {},
   "source": [
    "## 1.1 BERT Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1491d0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 9\n",
      "Accumulating gradients of 4 batches\n"
     ]
    }
   ],
   "source": [
    "model_size          = 'base'  # choose between 'tiny', 'base' and 'large'\n",
    "max_sequence_length = 256\n",
    "batch_size          = 9       # 10 should work on a 12 GB card if not also used for graphics / GUI\n",
    "virtual_batch_size  = 64\n",
    "\n",
    "# TODO: what virtual batch size should we use\n",
    "# https://arxiv.org/abs/1904.00962 are cited to say a large batch size (32k) is better\n",
    "# but they themselves warn that \"naively\" doing so can degrade performance\n",
    "\n",
    "max_epochs = 8\n",
    "limit_train_batches = 1.0     # fraction of training data to use\n",
    "\n",
    "hparams = {\n",
    "    \"encoder_learning_rate\": 1e-05,  # Encoder specific learning rate\n",
    "    \"learning_rate\":         3e-05,  # Classification head learning rate\n",
    "    \"nr_frozen_epochs\":      3,      # Number of epochs we want to keep the encoder model frozen\n",
    "    \"loader_workers\":        4,      # How many subprocesses to use for data loading.\n",
    "                                     # (0 means that the data will be loaded in the main process)\n",
    "    \"batch_size\":            batch_size,\n",
    "    \"gpus\":                  1,\n",
    "}  \n",
    "\n",
    "# compensate for small batch size with batch accumulation if needed\n",
    "accumulate_grad_batches = 1\n",
    "while batch_size * accumulate_grad_batches < virtual_batch_size:\n",
    "    # accumulated batch size too small\n",
    "    # --> accumulate more batches\n",
    "    accumulate_grad_batches += 1\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "if accumulate_grad_batches > 1:\n",
    "    print('Accumulating gradients of %d batches' %accumulate_grad_batches)\n",
    "    \n",
    "size2name = {\n",
    "    'tiny':  'distilbert-base-uncased',\n",
    "    'base':  'bert-base-uncased',\n",
    "    'large': 'bert-large-uncased',\n",
    "}\n",
    "\n",
    "model_name = size2name[model_size]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25048744",
   "metadata": {},
   "source": [
    "## 1.2 Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "050dedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data/ABSA16_Laptops_Train_SB1_v2.xml\n",
      "Using data/EN_LAPT_SB1_TEST_.xml.gold\n",
      "Using data/ABSA16_Restaurants_Train_SB1_v2.xml\n",
      "Using data/EN_REST_SB1_TEST.xml.gold\n"
     ]
    }
   ],
   "source": [
    "domains = ['laptop', 'restaurant']\n",
    "#domains = ['restaurant']\n",
    "\n",
    "train_dev_split = (95, 5)\n",
    "\n",
    "data_prefix = 'data/'\n",
    "\n",
    "filenames = {\n",
    "    'laptop':     ('ABSA16_Laptops_Train_SB1_v2.xml',\n",
    "                   'EN_LAPT_SB1_TEST_.xml.gold'),\n",
    "    'restaurant': ('ABSA16_Restaurants_Train_SB1_v2.xml',\n",
    "                   'EN_REST_SB1_TEST.xml.gold'),\n",
    "}\n",
    "\n",
    "for domain in domains:\n",
    "    for part in (0,1):\n",
    "        filename = data_prefix + filenames[domain][part]\n",
    "        print('Using', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6966290b",
   "metadata": {},
   "source": [
    "## 1.3 Question Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427dd405",
   "metadata": {},
   "outputs": [],
   "source": [
    "put_question_first = True  # whether to put question into seq A or B\n",
    "\n",
    "#use_questions_with_labeltype = ['polarity', 'yesno']\n",
    "#use_questions_with_labeltype = ['polarity']\n",
    "use_questions_with_labeltype = ['yesno']\n",
    "\n",
    "#use_question_with_description = 'Sun et al. QA-B'    # set to None to de-activate this filter\n",
    "use_question_with_description = None\n",
    "\n",
    "templates = [\n",
    "    \n",
    "    # Hoang et al. (2019)\n",
    "    {   'question': '%(entity_type)s, %(attribute_label)s',\n",
    "        'label':    '%(polarity)s',\n",
    "        'description': 'Hoang et al.',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 1\n",
    "    {   'question': '%(entity_type)s - %(attribute_label)s',\n",
    "        'label':    '%(polarity)s',\n",
    "        'description': 'Sun et al. NLI-M',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 2 \"NLI-M\"\n",
    "    {    'question': 'What do you think of the %(attribute_label)s of %(entity_type)s?',\n",
    "         'label':    '%(polarity)s',\n",
    "         'description': 'Sun et al. QA-M',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 3 \"QA-B\"\n",
    "    {    'question': 'The polarity of the aspect %(attribute_label)s of %(entity_type)s is %(candidate_polarity)s.',\n",
    "         'label':    '%(yesno)s',\n",
    "         'description': 'Sun et al. QA-B',\n",
    "    },\n",
    "    \n",
    "    # Sun et al. (2019) format 4\n",
    "    {    'question': '%(entity_type)s - %(attribute_label)s - %(candidate_polarity)s',\n",
    "         'label':    '%(yesno)s',\n",
    "         'description': 'Sun et al. NLI-B',\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a22972dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates += [\n",
    "    \n",
    "    # Variant 1\n",
    "    {    'question': 'In terms of %(attribute_label)s, what do you think of %(entity_type)s?',\n",
    "         'label':    '%(polarity)s',\n",
    "         'description': 'Variant 1, QA-M',\n",
    "    },\n",
    "    \n",
    "    # Variant 2\n",
    "    {    'question': 'What polarity has the sentiment towards the %(attribute_label)s of %(entity_type)s in the following rewview?',\n",
    "         'label':    '%(polarity)s',\n",
    "         'description': 'Variant 2, QA-M',\n",
    "    },\n",
    "    \n",
    "    # Variant 3\n",
    "    {    'question': 'Do you agree that the sentiment towards the aspect %(attribute_label)s of %(entity_type)s in the following review is %(candidate_polarity)s?',\n",
    "         'label':    '%(yesno)s',\n",
    "         'description': 'Variant 3, QA-B',\n",
    "    },\n",
    "    \n",
    "    # Variant 4\n",
    "    {    'question': 'Is there %(candidate_polarity)s sentiment towards the %(attribute_label)s aspect of %(entity_type)s?',\n",
    "         'label':    '%(yesno)s',\n",
    "         'description': 'Variant 4, QA-B',\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# TODO: add variants with entity type and attribute label not in ALLCAPS and\n",
    "#       with _ replaced with whitespace (requires additional code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "860c51dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 template(s) selected\n"
     ]
    }
   ],
   "source": [
    "# remove templates with wrong question type\n",
    "\n",
    "filtered_templates = []\n",
    "for template in templates:\n",
    "    use_template = False\n",
    "    for labeltype in use_questions_with_labeltype:\n",
    "        if labeltype in template['label']:\n",
    "            use_template = True\n",
    "            break\n",
    "    if use_question_with_description:\n",
    "        if use_question_with_description != template['description']:\n",
    "            use_template = False\n",
    "    if use_template:\n",
    "        filtered_templates.append(template)\n",
    "templates = filtered_templates\n",
    "print(len(templates), 'template(s) selected')\n",
    "\n",
    "\n",
    "# If there are multiple domains it probably helps to include the domain in the question\n",
    "# TODO: empirically test this idea\n",
    "\n",
    "if len(domains) > 1:    \n",
    "    question_prefix = '%(domain)s: '\n",
    "    for template in templates:\n",
    "        template['question'] = question_prefix + template['question']\n",
    "        \n",
    "assert len(templates) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78784a7",
   "metadata": {},
   "source": [
    "## 2.1 Get Data Instances from XML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849419f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 5416\n",
      "\n",
      "observed entity types: ['AMBIENCE', 'BATTERY', 'COMPANY', 'CPU', 'DISPLAY', 'DRINKS', 'FANS_COOLING', 'FOOD', 'GRAPHICS', 'HARDWARE', 'HARD_DISC', 'KEYBOARD', 'LAPTOP', 'LOCATION', 'MEMORY', 'MOTHERBOARD', 'MOUSE', 'MULTIMEDIA_DEVICES', 'OPTICAL_DRIVES', 'OS', 'PORTS', 'POWER_SUPPLY', 'RESTAURANT', 'SERVICE', 'SHIPPING', 'SOFTWARE', 'SUPPORT', 'WARRANTY']\n",
      "\n",
      "observed attribute labels: ['CONNECTIVITY', 'DESIGN_FEATURES', 'GENERAL', 'MISCELLANEOUS', 'OPERATION_PERFORMANCE', 'PORTABILITY', 'PRICE', 'PRICES', 'QUALITY', 'STYLE_OPTIONS', 'USABILITY']\n",
      "\n",
      "observed polarities: ['negative', 'neutral', 'positive']\n",
      "\n",
      "number of unique targets: 721\n"
     ]
    }
   ],
   "source": [
    "# mostly implemented from scratch, some inspiration from\n",
    "# https://opengogs.adaptcentre.ie/rszk/sea/src/master/lib/semeval_absa.py\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    filename, domain,\n",
    "    observed_entity_types, observed_attribute_labels,\n",
    "    observed_polarities,   observed_targets\n",
    "):\n",
    "    xmltree = ElementTree.parse(filename)\n",
    "    xmlroot = xmltree.getroot()\n",
    "    dataset = []\n",
    "    for sentence in xmlroot.iter('sentence'):\n",
    "        sent_id = sentence.get('id')\n",
    "        # get content inside the first <text>...</text> sub-element\n",
    "        text = sentence.findtext('text').strip()\n",
    "        #print(sent_id, text)\n",
    "        for opinion in sentence.iter('Opinion'):\n",
    "            opin_cat = opinion.get('category')\n",
    "            entity_type, attribute_label = opin_cat.split('#')\n",
    "            polarity = opinion.get('polarity')\n",
    "            target = opinion.get('target')\n",
    "            try:\n",
    "                span = (int(opinion.get('from')), int(opinion.get('to')))\n",
    "            except TypeError:\n",
    "                # at least one of 'from' or 'to' is missing\n",
    "                span = (0, 0)\n",
    "            if target == 'NULL':\n",
    "                target = None\n",
    "            # add to dataset\n",
    "            dataset.append((\n",
    "                domain,\n",
    "                sent_id, text,\n",
    "                entity_type, attribute_label,\n",
    "                target, span,\n",
    "                polarity\n",
    "            ))\n",
    "            # update vocabularies\n",
    "            observed_entity_types.add(entity_type)\n",
    "            observed_attribute_labels.add(attribute_label)\n",
    "            observed_polarities.add(polarity)\n",
    "            if target:\n",
    "                observed_targets.add(target)\n",
    "    return dataset\n",
    "\n",
    "tr_observed_entity_types = set()\n",
    "tr_observed_attribute_labels = set()\n",
    "tr_observed_polarities = set()\n",
    "tr_observed_targets = set()\n",
    "\n",
    "tr_dataset = []\n",
    "for domain in domains:\n",
    "    filename = data_prefix + filenames[domain][0]\n",
    "    tr_dataset += get_dataset(\n",
    "        filename, domain,\n",
    "        tr_observed_entity_types, tr_observed_attribute_labels,\n",
    "        tr_observed_polarities,   tr_observed_targets\n",
    "    )\n",
    "\n",
    "print('dataset size:', len(tr_dataset))\n",
    "print('\\nobserved entity types:',     sorted(tr_observed_entity_types))\n",
    "print('\\nobserved attribute labels:', sorted(tr_observed_attribute_labels))\n",
    "print('\\nobserved polarities:',       sorted(tr_observed_polarities))\n",
    "print('\\nnumber of unique targets:',  len(tr_observed_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73d21d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 1660\n",
      "\n",
      "new observed entity types: []\n",
      "\n",
      "new observed attribute labels: []\n",
      "\n",
      "new observed polarities: []\n",
      "\n",
      "number of unique targets: 312\n"
     ]
    }
   ],
   "source": [
    "# get test data\n",
    "\n",
    "te_observed_entity_types = set()\n",
    "te_observed_attribute_labels = set()\n",
    "te_observed_polarities = set()\n",
    "te_observed_targets = set()\n",
    "\n",
    "te_dataset = []\n",
    "for domain in domains:\n",
    "    filename = data_prefix + filenames[domain][1]\n",
    "    te_dataset += get_dataset(\n",
    "        filename, domain,\n",
    "        te_observed_entity_types, te_observed_attribute_labels,\n",
    "        te_observed_polarities,   te_observed_targets\n",
    "    )\n",
    "\n",
    "print('dataset size:', len(te_dataset))\n",
    "print('\\nnew observed entity types:',     sorted(te_observed_entity_types - tr_observed_entity_types))\n",
    "print('\\nnew observed attribute labels:', sorted(te_observed_attribute_labels - tr_observed_attribute_labels))\n",
    "print('\\nnew observed polarities:',       sorted(te_observed_polarities - tr_observed_polarities))\n",
    "print('\\nnumber of unique targets:',  len(te_observed_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e483862",
   "metadata": {},
   "source": [
    "## 2.2 Training-Dev Split\n",
    "The SemEval ABSA dataset comes without a dev set. We need a dev set to decide how long to train, to select other parameters and to select a good run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68c11369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('laptop', 'positive'): split 1555 (95.0%) to 82 (5.0%)\n",
      "('laptop', 'negative'): split 1029 (94.9%) to 55 (5.1%)\n",
      "('laptop', 'neutral'): split 178 (94.7%) to 10 (5.3%)\n",
      "('restaurant', 'negative'): split 711 (94.9%) to 38 (5.1%)\n",
      "('restaurant', 'positive'): split 1574 (95.0%) to 83 (5.0%)\n",
      "('restaurant', 'neutral'): split 95 (94.1%) to 6 (5.9%)\n",
      "\n",
      "Training data size: 5142\n",
      "Development data size: 274\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# how many instances are there for each label?\n",
    "\n",
    "group2indices = {}\n",
    "for index, item in enumerate(tr_dataset):\n",
    "    domain   = item[0]\n",
    "    polarity = item[-1]\n",
    "    group = (domain, polarity)\n",
    "    if not group in group2indices:\n",
    "        group2indices[group] = []\n",
    "    group2indices[group].append(index)\n",
    "\n",
    "# create stratified sample\n",
    "    \n",
    "rel_train_size, rel_dev_size = train_dev_split  # configured in section 1.2\n",
    "rel_total = rel_train_size + rel_dev_size\n",
    "\n",
    "tr_indices = []\n",
    "dev_indices = []\n",
    "\n",
    "for group in group2indices:\n",
    "    indices = group2indices[group]\n",
    "    n = len(indices)\n",
    "    select = (n * rel_train_size) // rel_total\n",
    "    remaining = n - select\n",
    "    print('%r: split %d (%.1f%%) to %d (%.1f%%)' %(\n",
    "        group, select, 100.0*select/float(n),\n",
    "        remaining, 100.0*remaining/float(n),\n",
    "    ))\n",
    "    random.shuffle(indices)\n",
    "    tr_indices += indices[:select]\n",
    "    dev_indices += indices[select:]\n",
    "\n",
    "tr_indices.sort()\n",
    "dev_indices.sort()\n",
    "\n",
    "def get_subset(dataset, indices):  # TODO: this probably can be replaced with [] slicing\n",
    "    retval = []\n",
    "    for index in indices:\n",
    "        retval.append(dataset[index])\n",
    "    return retval\n",
    "    \n",
    "dev_dataset = get_subset(tr_dataset, dev_indices)\n",
    "tr_dataset  = get_subset(tr_dataset, tr_indices)\n",
    "\n",
    "print()\n",
    "print('Training data size:', len(tr_dataset))\n",
    "print('Development data size:', len(dev_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f083d49",
   "metadata": {},
   "source": [
    "## 2.3 PyTorch DataLoader\n",
    "\n",
    "To use the PyTorch Lighting framwork, we need to distinguish 3 types of objects handling our data:\n",
    "\n",
    "### Dataset\n",
    "\n",
    "PyTorch Dataset objects provide access to a data set and behave like a list of dictionaries, one dictionary for each data instance (training or test item). The framework does not prescribe what the dictionaries look like, i.e. you can choose the keys. The length of the list determines the number of training instances in each epoch, unless the DataLoader (below) is extended to filter or augment the data. The standard way to augment data is to keep the number of instances identical to the number of raw instances and to apply a different or random transformation in each call of `__getitem__()`.\n",
    "\n",
    "### DataLoader\n",
    "\n",
    "PyTorch DataLoader objects shuffle data provided by a Dataset object and create batches of data.\n",
    "\n",
    "### LightningDataModule\n",
    "\n",
    "LightningDataModule objects create 3 DataLoader objects, one each for training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic usage of pytorch and lightning from\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "# and\n",
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "class ABSA_Dataset_part_1(Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_data,\n",
    "        put_question_first = True,\n",
    "        template_index = -1,    # -1 = pick random template\n",
    "        info = None,            # additional info to keep with each instance\n",
    "    ):\n",
    "        self.raw_data            = raw_data\n",
    "        self.put_question_first  = put_question_first\n",
    "        self.template_index      = template_index\n",
    "        self.info                = info\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' get one instance of the dataset as a custom dictionary\n",
    "        '''\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            assert isinstance(idx, int)\n",
    "        domain, sent_id, text, \\\n",
    "            entity_type, attribute_label, \\\n",
    "            target, span, \\\n",
    "            polarity = self.raw_data[idx]\n",
    "        question, label = self.pick_question(\n",
    "            entity_type, attribute_label, domain, polarity,\n",
    "        )\n",
    "        # TODO: support adding context (previous sentences) to text\n",
    "        retval = {}\n",
    "        if self.put_question_first:\n",
    "            retval['seq_A'] = question\n",
    "            retval['seq_B'] = text\n",
    "        else:\n",
    "            retval['seq_A'] = text\n",
    "            retval['seq_B'] = question\n",
    "        retval['label'] = label\n",
    "        retval['domain'] = domain\n",
    "        retval['info']  = self.info\n",
    "        return retval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f674226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ABSA_Dataset(ABSA_Dataset_part_1):\n",
    "                   \n",
    "    def pick_question(self, entity_type, attribute_label, domain, polarity):\n",
    "        global templates\n",
    "        global observed_polarities\n",
    "        if self.template_index < 0:\n",
    "            template = random.choice(templates)\n",
    "        else:\n",
    "            template = templates[self.template_index]\n",
    "        candidate_polarity = random.choice(list(tr_observed_polarities))\n",
    "        if candidate_polarity == polarity:\n",
    "            yesno = 'yes'\n",
    "        else:\n",
    "            yesno = 'no'\n",
    "        question = template['question'] %locals()\n",
    "        label    = template['label']    %locals()\n",
    "        return (question, label)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ecda307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 5142\n",
      "Devset size (using all templates): 1096\n"
     ]
    }
   ],
   "source": [
    "# wrap training and dev data\n",
    "\n",
    "tr_dataset_object = ABSA_Dataset(\n",
    "    tr_dataset,\n",
    "    put_question_first = put_question_first,\n",
    "    template_index = -1,  # pick question at random\n",
    ")\n",
    "\n",
    "print('Training size:', len(tr_dataset_object))\n",
    "\n",
    "# create a separate dev set for each question template\n",
    "\n",
    "dev_dataset_objects = []\n",
    "for template_index in range(len(templates)):\n",
    "    dev_dataset_objects.append(ABSA_Dataset(\n",
    "        dev_dataset,\n",
    "        put_question_first = put_question_first,\n",
    "        template_index = template_index,\n",
    "    ))\n",
    "\n",
    "# also provide a dev set that is the union of the above dev sets,\n",
    "# e.g. to be used for model selection\n",
    "\n",
    "dev_dataset_combined = torch.utils.data.ConcatDataset(dev_dataset_objects)\n",
    "\n",
    "print('Devset size (using all templates):', len(dev_dataset_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696d4be",
   "metadata": {},
   "source": [
    "## 2.4 Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f092c3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size (using all templates): 6640\n"
     ]
    }
   ],
   "source": [
    "# create a separate test set for each question template\n",
    "\n",
    "te_dataset_objects = []\n",
    "for template_index in range(len(templates)):\n",
    "    te_dataset_objects.append(ABSA_Dataset(\n",
    "        te_dataset,\n",
    "        put_question_first = put_question_first,\n",
    "        template_index = template_index,\n",
    "    ))\n",
    "\n",
    "# also provide a test set that is the union of the above test sets\n",
    "\n",
    "te_dataset_combined = torch.utils.data.ConcatDataset(te_dataset_objects)\n",
    "\n",
    "print('Test set size (using all templates):', len(te_dataset_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7193f8b",
   "metadata": {},
   "source": [
    "## 2.5 Lightning Wrapper for Training, Development and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "800185d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ricardorei/lightning-text-classification/blob/master/classifier.py\n",
    "    \n",
    "import pytorch_lightning as pl\n",
    "from torchnlp.encoders import LabelEncoder\n",
    "\n",
    "class ABSA_DataModule(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, classifier, data_split = None, **kwargs):\n",
    "        global use_questions_with_labeltype\n",
    "        super().__init__()\n",
    "        self.hparams.update(classifier.hparams)\n",
    "        self.classifier = classifier\n",
    "        if data_split is None:      # this happens when loading a checkpoint\n",
    "            data_split = (None, None, None)\n",
    "        self.data_split = data_split\n",
    "        self.kwargs = kwargs\n",
    "        labelset = set()\n",
    "        if 'polarity' in use_questions_with_labeltype:\n",
    "            for label in tr_observed_polarities:\n",
    "                labelset.add(label)\n",
    "        if 'yesno' in use_questions_with_labeltype:\n",
    "            labelset.add('yes')\n",
    "            labelset.add('no')\n",
    "        print('Labelset:', labelset)\n",
    "        self.label_encoder = LabelEncoder(\n",
    "            sorted(list(labelset)),\n",
    "            reserved_labels = [],\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the training data '''\n",
    "        dataset = self.data_split[0]\n",
    "        return DataLoader(\n",
    "            dataset     = dataset,\n",
    "            sampler     = RandomSampler(dataset),\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the validation data '''\n",
    "        return DataLoader(\n",
    "            dataset     = self.data_split[1],\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        ''' create a data loader for the test data '''\n",
    "        return DataLoader(\n",
    "            dataset     = self.data_split[2],\n",
    "            batch_size  = self.hparams.batch_size,\n",
    "            collate_fn  = self.classifier.prepare_sample,\n",
    "            num_workers = self.hparams.loader_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e00ce",
   "metadata": {},
   "source": [
    "## 3.1 Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a9e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_part_1(pl.LightningModule):\n",
    "    \n",
    "    #def __init__(self, hparams = None, **kwargs) -> None:\n",
    "    def __init__(self, hparams = None, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        if type(hparams) is dict:\n",
    "            #print('Converting', type(hparams))\n",
    "            hparams = pl.utilities.AttributeDict(hparams)\n",
    "        #print('New classifier with', hparams)\n",
    "        # https://discuss.pytorch.org/t/pytorch-lightning-module-cant-set-attribute-error/121125\n",
    "        if hparams:\n",
    "            self.hparams.update(hparams)\n",
    "        self.add_missing_hparams()\n",
    "        hparams = self.hparams\n",
    "        self.batch_size = hparams.batch_size\n",
    "        self.data = ABSA_DataModule(self, **kwargs)\n",
    "        if 'tokeniser' in kwargs:\n",
    "            self.tokenizer = kwargs['tokeniser']  # attribute expected by lightning\n",
    "        else:\n",
    "            # this happens when loading a checkpoint\n",
    "            self.tokenizer = None  # TODO: this may break ability to use the model\n",
    "        self.__build_model()\n",
    "        self.__build_loss()\n",
    "        # prepare training with frozen BERT layers so that the new\n",
    "        # classifier head can first adjust to BERT before BERT\n",
    "        # adjusts to the classifier in later epochs        \n",
    "        if hparams.nr_frozen_epochs > 0:\n",
    "            self.freeze_encoder()\n",
    "        else:\n",
    "            self._frozen = False\n",
    "        self.nr_frozen_epochs = hparams.nr_frozen_epochs\n",
    "        self.record_predictions = False\n",
    "            \n",
    "    def __build_model(self) -> None:\n",
    "        ''' Init BERT model, tokeniser and classification head '''\n",
    "        # Q: Why not use AutoModelForSequenceClassification?\n",
    "        self.bert = AutoModel.from_pretrained(\n",
    "            model_name,  # was: self.hparams.encoder_model\n",
    "            output_hidden_states = True\n",
    "        )\n",
    "        # parameters for the classification head: best values\n",
    "        # depend on the task and dataset; the below values\n",
    "        # have not been tuned much but work reasonable well\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.bert.config.hidden_size, 1536),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1536, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, self.data.label_encoder.vocab_size)\n",
    "        )\n",
    "        \n",
    "    def __build_loss(self):\n",
    "        self._loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8780c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "\n",
    "class Classifier_part_2(Classifier_part_1):\n",
    "    \n",
    "    def unfreeze_encoder(self) -> None:\n",
    "        if self._frozen:\n",
    "            log.info('\\n== Encoder model fine-tuning ==')\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = True\n",
    "            self._frozen = False\n",
    "            \n",
    "    def freeze_encoder(self) -> None:\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._frozen = True\n",
    "\n",
    "    def predict(self, sample: dict) -> dict:\n",
    "        ''' make a prediction for a single data instance '''\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_inputs, _ = self.prepare_sample(\n",
    "                [sample],\n",
    "                prepare_target = False\n",
    "            )\n",
    "            model_out = self.forward(batch_inputs)\n",
    "            logits = torch.Tensor.cpu(model_out[\"logits\"]).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits, axis=1)\n",
    "            ]\n",
    "            sample[\"predicted_label\"] = predicted_labels[0]\n",
    "        return sample\n",
    "    \n",
    "    # functionality to obtain predictions for a dataset as a\n",
    "    # side effect of asking PyTorch Lightning to get evaluation\n",
    "    # results for a dataset\n",
    "    # (the framework does not seem to provide a function to get\n",
    "    # all predictions for a dataset)\n",
    "    \n",
    "    def start_recording_predictions(self):\n",
    "        self.record_predictions = True\n",
    "        self.reset_recorded_predictions()\n",
    "        \n",
    "    def stop_recording_predictions(self):\n",
    "        self.record_predictions = False\n",
    "        \n",
    "    def reset_recorded_predictions(self):\n",
    "        self.seq2label = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cea6c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.utils import lengths_to_mask\n",
    "\n",
    "class Classifier_part_3(Classifier_part_2):\n",
    "    \n",
    "    def forward(self, batch_input):\n",
    "        tokens  = batch_input['input_ids']\n",
    "        lengths = batch_input['length']\n",
    "        mask = batch_input['attention_mask']\n",
    "        # Run BERT model.\n",
    "        word_embeddings = self.bert(tokens, mask).last_hidden_state\n",
    "        sentemb = word_embeddings[:,0]  # at position of [CLS]\n",
    "        logits = self.classification_head(sentemb)\n",
    "        # Hack to conveniently use the model and trainer to\n",
    "        # get predictions for a test set:\n",
    "        if self.record_predictions:\n",
    "            logits_np = torch.Tensor.cpu(logits).numpy()\n",
    "            predicted_labels = [\n",
    "                self.data.label_encoder.index_to_token[prediction]\n",
    "                for prediction in numpy.argmax(logits_np, axis=1)\n",
    "            ]\n",
    "            for index, input_token_ids in enumerate(tokens):\n",
    "                key = torch.Tensor.cpu(input_token_ids).numpy().tolist()\n",
    "                # truncate trailing zeros\n",
    "                while key and key[-1] == 0:\n",
    "                    del key[-1]\n",
    "                self.seq2label[tuple(key)] = predicted_labels[index]\n",
    "        return {\"logits\": logits}\n",
    "    \n",
    "    def loss(self, predictions: dict, targets: dict) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Computes Loss value according to a loss function.\n",
    "        :param predictions: model specific output. Must contain a key 'logits' with\n",
    "            a tensor [batch_size x 1] with model predictions\n",
    "        :param labels: Label values [batch_size]\n",
    "        Returns:\n",
    "            torch.tensor with loss value.\n",
    "        \"\"\"\n",
    "        return self._loss(predictions[\"logits\"], targets[\"labels\"])\n",
    "    \n",
    "    def add_missing_hparams(self):\n",
    "        # fix various attribute errors when instantiating via\n",
    "        # load_from_checkpoint()\n",
    "        # like self.hparams.update() but do not overwrite \n",
    "        # values already set\n",
    "        # TODO: check docs whether there is a parameter to\n",
    "        #       request this behaviour from update()\n",
    "        global hparams\n",
    "        for key in hparams:\n",
    "            if not hasattr(self.hparams, key):\n",
    "                self.hparams[key] = hparams[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01a510a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Classifier_part_4(Classifier_part_3):\n",
    "    \n",
    "    def prepare_sample(self, sample: list, prepare_target: bool = True) -> (dict, dict):\n",
    "        \"\"\" prepare a batch of instances to pass them into the model\n",
    "        \n",
    "        :param sample: list of dictionaries.\n",
    "        \n",
    "        Returns:\n",
    "            - dictionary with the expected model inputs.\n",
    "            - dictionary with the expected target labels.\n",
    "        \"\"\"\n",
    "        assert len(sample) <= batch_size\n",
    "        assert self.tokenizer is not None\n",
    "        batch_seq_A = []\n",
    "        batch_seq_B = []\n",
    "        for item in sample:\n",
    "            batch_seq_A.append(item['seq_A'])\n",
    "            batch_seq_B.append(item['seq_B'])\n",
    "        # run the tokeniser\n",
    "        encoded_batch = self.tokenizer(\n",
    "            batch_seq_A,\n",
    "            batch_seq_B,\n",
    "            is_split_into_words = False,\n",
    "            return_length       = True,\n",
    "            padding             = 'max_length',\n",
    "            # https://github.com/huggingface/transformers/issues/8691\n",
    "            return_tensors      = 'pt',\n",
    "        )\n",
    "        if not prepare_target:\n",
    "            return encoded_batch, {}  # no target labels requested\n",
    "        # Prepare target:\n",
    "        batch_labels = []\n",
    "        for item in sample:\n",
    "            batch_labels.append(item['label'])\n",
    "        assert len(batch_labels) <= batch_size\n",
    "        try:\n",
    "            targets = {\n",
    "                \"labels\": self.data.label_encoder.batch_encode(batch_labels)\n",
    "            }\n",
    "            return encoded_batch, targets\n",
    "        except RuntimeError:\n",
    "            raise Exception(\"Label encoder found an unknown label.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caf5059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Classifier_part_5(Classifier_part_4):\n",
    "    \n",
    "    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        ''' perform a training step with the given batch '''\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        # Q: What is this about?\n",
    "        # attribute no longer exists\n",
    "        #if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "        #    loss_val = loss_val.unsqueeze(0)\n",
    "        output = OrderedDict({\"loss\": loss_val})\n",
    "        self.log('train_loss', loss_val, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        # can also return just a scalar instead of a dict (return loss_val)\n",
    "        return output\n",
    "   \n",
    "    def test_or_validation_step(self, test_type, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        ''' perform a test or validation step with the given batch '''\n",
    "        inputs, targets = batch\n",
    "        model_out = self.forward(inputs)\n",
    "        loss_val = self.loss(model_out, targets)\n",
    "        y = targets[\"labels\"]\n",
    "        # get predictions\n",
    "        y_hat = model_out[\"logits\"]\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        # get accuracy\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "        if self.on_gpu:\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        # attribute no longer exists\n",
    "        #if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "        #    loss_val = loss_val.unsqueeze(0)\n",
    "        #    val_acc = val_acc.unsqueeze(0)\n",
    "        output = OrderedDict({\n",
    "            test_type + \"_loss\": loss_val,\n",
    "            test_type + \"_acc\":  val_acc,\n",
    "            'batch_size': len(batch),\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'val', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "    \n",
    "    def test_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:\n",
    "        return self.test_or_validation_step(\n",
    "            'test', batch, batch_nb, *args, **kwargs\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1e8cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "class Classifier(Classifier_part_5):\n",
    "    \n",
    "    # validation_end() is now validation_epoch_end()\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/blob/efd272a3cac2c412dd4a7aa138feafb2c114326f/CHANGELOG.md\n",
    "    \n",
    "    def test_or_validation_epoch_end(self, test_type, outputs: list) -> None:\n",
    "        ''' calculate average loss and accuracy over all batches,\n",
    "            reducing the weight of the last batch according to its\n",
    "            size so that all data instances have equal influence\n",
    "            on the scores\n",
    "        '''\n",
    "        val_loss_mean = 0.0\n",
    "        val_acc_mean = 0.0\n",
    "        total_size = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output[test_type + \"_loss\"]\n",
    "            # reduce manually when using dp\n",
    "            # -- attribute no longer exists\n",
    "            #if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            #    val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output[test_type + \"_acc\"]\n",
    "            #if self.trainer.use_dp or self.trainer.use_ddp2:\n",
    "            #    val_acc = torch.mean(val_acc)\n",
    "            # We weight the batch accuracy by batch size to not give\n",
    "            # higher weight to the items of a smaller, final bacth.\n",
    "            batch_size = output['batch_size']\n",
    "            val_acc_mean += val_acc * batch_size\n",
    "            total_size += batch_size\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= total_size\n",
    "        self.log(test_type+'_loss', val_loss_mean)\n",
    "        self.log(test_type+'_acc',  val_acc_mean)\n",
    "\n",
    "    def validation_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('val', outputs)\n",
    "                                     \n",
    "    def test_epoch_end(self, outputs: list) -> None:\n",
    "        self.test_or_validation_epoch_end('test', outputs)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Sets different Learning rates for different parameter groups. \"\"\"\n",
    "        parameters = [\n",
    "            {\"params\": self.classification_head.parameters()},\n",
    "            {\n",
    "                \"params\": self.bert.parameters(),\n",
    "                \"lr\": self.hparams.encoder_learning_rate,\n",
    "                #\"weight_decay\": 0.01,  # TODO: try this as it is in the BERT paper\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.Adam(parameters, lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Pytorch lightning hook \"\"\"\n",
    "        if self.current_epoch + 1 >= self.nr_frozen_epochs:\n",
    "            self.unfreeze_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3434032b",
   "metadata": {},
   "source": [
    "## 4.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113178f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(\n",
    "    hparams = hparams,    \n",
    "    # parameters for ABSA_DataModule:\n",
    "    data_split = (tr_dataset_object, dev_dataset_combined, te_dataset_combined),\n",
    "    # additional required parameters:\n",
    "    tokeniser  = tokeniser\n",
    ")   \n",
    "print('Ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4603b090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                | Type             | Params\n",
      "---------------------------------------------------------\n",
      "0 | bert                | BertModel        | 109 M \n",
      "1 | classification_head | Sequential       | 1.6 M \n",
      "2 | _loss               | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "111 M     Total params\n",
      "444.230   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef0d3e77ab94fc89f2d13acdf12a3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 47 minutes\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/early_stopping.html\n",
    "\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import os\n",
    "import time\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor   = 'val_acc',\n",
    "    min_delta = 0.00,\n",
    "    patience  = 7,\n",
    "    verbose   = False,\n",
    "    mode      = 'max',\n",
    ")\n",
    "\n",
    "save_top_model_callback = ModelCheckpoint(\n",
    "    save_top_k = 3,\n",
    "    monitor    = 'val_acc',\n",
    "    mode       = 'max',\n",
    "    filename   = '{val_acc:.4f}-{epoch:02d}-{val_loss:.4f}'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[early_stop_callback, save_top_model_callback],\n",
    "    max_epochs = max_epochs,\n",
    "    min_epochs = classifier.hparams.nr_frozen_epochs + 2,\n",
    "    gpus = classifier.hparams.gpus,\n",
    "    accumulate_grad_batches = accumulate_grad_batches,\n",
    "    limit_train_batches     = limit_train_batches,\n",
    "    check_val_every_n_epoch = 1,\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "start = time.time()\n",
    "trainer.fit(classifier, classifier.data)\n",
    "print('Training time: %.0f minutes' %((time.time()-start)/60.0))\n",
    "\n",
    "## Appendix A: Example BERT Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d84bb2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3239), started 1 day, 15:54:36 ago. (Use '!kill 3239' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a0a23bcb3f66c787\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a0a23bcb3f66c787\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=nCq_vy9qE-k at 44:59\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28a77654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is /home/jwagner/sentiment-2021/absa-rationale-eval/notebooks/lightning_logs/default/version_2/checkpoints/val_acc=0.9336-epoch=07-val_loss=0.1989.ckpt\n",
      "Best validation set accuracy: tensor(0.9336, device='cuda:0')\n",
      "Test results via trainer.test():\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jwagner/sentiment-2021/absa-rationale-eval/venv-pytorch/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.prepare_data has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.prepare_data.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca786a888ca94fa7bc0e648c3f4d003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8995469212532043, 'test_loss': 0.2787647247314453}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('The best model is', save_top_model_callback.best_model_path)\n",
    "\n",
    "print('Best validation set accuracy:', save_top_model_callback.best_model_score)\n",
    "\n",
    "# The following automatically loads the best weights according to\n",
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html\n",
    "\n",
    "print('Test results via trainer.test():')\n",
    "results = trainer.test()  # also prints results as a side effect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c6d8f",
   "metadata": {},
   "source": [
    "## 5.1 Save Best Model outside Logs\n",
    "\n",
    "Rather than manually locating the best model in the lightning logs folder and copying it to another location, use the library to save a copy. This also gives us the option to save a copy without the training state of the Adam optimiser, reducing model size by about 67%, training parameters and filesystem paths that we may not want to share with users of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a6d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# https://pytorch-lightning.readthedocs.io/en/latest/common/weights_loading.html\n",
    "\n",
    "# after just having run test(), the best checkpoint is still loaded but that's\n",
    "# not a documented feature so to be on the safe side for future versions we\n",
    "# need to explicitly load the best checkpoint:\n",
    "\n",
    "best_model = Classifier.load_from_checkpoint(\n",
    "    checkpoint_path = trainer.checkpoint_callback.best_model_path\n",
    "    # the hparams including hparams.batch_size appear to have been\n",
    "    # saved in the checkpoint automatically\n",
    ")\n",
    "# best_model.save_checkpoint('best.ckpt') does not exist\n",
    "# --> need to wrap model into trainer to be able to save a checkpoint\n",
    "\n",
    "new_trainer = pl.Trainer(\n",
    "    resume_from_checkpoint = trainer.checkpoint_callback.best_model_path,\n",
    "    gpus = -1,  # avoid warnings (-1 = automatic selection)\n",
    "    # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    ")\n",
    "new_trainer.model = best_model  # @model.setter in plugins/training_type/training_type_plugin.py\n",
    "\n",
    "new_trainer.save_checkpoint(\n",
    "    \"best-model-weights-only.ckpt\",\n",
    "    True,  # save_weights_only\n",
    "    # (if saved with setting the 2nd arg to True, the checkpoint   # TODO: \"False\"?\n",
    "    # will contain absoulte paths and training parameters)\n",
    ")\n",
    "\n",
    "# to just save the bert model in pytorch format and without the classification head, we follow\n",
    "# https://github.com/PyTorchLightning/pytorch-lightning/issues/3096#issuecomment-686877242\n",
    "best_model.bert.save_pretrained('best-bert-encoder.pt')\n",
    "\n",
    "# TODO: the above only saves the BERT encoder, not the classification head\n",
    "\n",
    "# Since the lightning module inherits from pytorch, we can save the full network in\n",
    "# pytorch format:\n",
    "torch.save(best_model.state_dict(), 'best-model.pt')\n",
    "\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ddb62",
   "metadata": {},
   "source": [
    "## 5.2 Load and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbd98519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None)\n",
      "\n",
      "Question template 0: '%(domain)s: The polarity of the aspect %(attribute_label)s of %(entity_type)s is %(candidate_polarity)s.'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8726ffc55d6a44c6be96c9b9c5811b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9072287678718567, 'test_loss': 0.2512328028678894}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Breakdown by domain:\n",
      "laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a66bf1b2b0a4520a8fc616af7aced45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8839507699012756, 'test_loss': 0.31210780143737793}\n",
      "--------------------------------------------------------------------------------\n",
      "restaurant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e61081a0ad44705b9ec39ea23bb8c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9136954545974731, 'test_loss': 0.2449449896812439}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question template 1: '%(domain)s: %(entity_type)s - %(attribute_label)s - %(candidate_polarity)s'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bc61096a354490966e68cdf781420d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9048192501068115, 'test_loss': 0.26398128271102905}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breakdown by domain:\n",
      "laptop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a494af4d894d22bd9ad8e7d0add887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8716050386428833, 'test_loss': 0.34375420212745667}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a153970b94b49bf67e3362d4f8d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9067185521125793, 'test_loss': 0.25523918867111206}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Question template 2: '%(domain)s: Do you agree that the sentiment towards the aspect %(attribute_label)s of %(entity_type)s in the following review is %(candidate_polarity)s?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2238ffa2744a0382809bce12f0013f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8963854908943176, 'test_loss': 0.27313998341560364}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Breakdown by domain:\n",
      "laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ce4b00e3144f7a93feb6be5569e401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8691360354423523, 'test_loss': 0.33897823095321655}\n",
      "--------------------------------------------------------------------------------\n",
      "restaurant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f501001821fb43f4b70f76688fea8261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9114989042282104, 'test_loss': 0.24122606217861176}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Question template 3: '%(domain)s: Is there %(candidate_polarity)s sentiment towards the %(attribute_label)s aspect of %(entity_type)s?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325da185cb6846afa14b0b8cb5068ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8981927037239075, 'test_loss': 0.2904355525970459}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Breakdown by domain:\n",
      "laptop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945edeac906d4952b59b0071eb94fc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.8913582563400269, 'test_loss': 0.3060298562049866}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restaurant\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ae1d6ef2e94b1b8c7d42c91f6f4e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9182174205780029, 'test_loss': 0.2391718178987503}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Summary:\n",
      "Q\tOverall\tLaptop\tRestaurant\tDescription\n",
      "0\t90.7\t88.4\t91.4\tSun et al. QA-B\n",
      "1\t90.5\t87.2\t90.7\tSun et al. NLI-B\n",
      "2\t89.6\t86.9\t91.1\tVariant 3, QA-B\n",
      "3\t89.8\t89.1\t91.8\tVariant 4, QA-B\n"
     ]
    }
   ],
   "source": [
    "best_model = Classifier.load_from_checkpoint(\n",
    "    checkpoint_path = 'best-model-weights-only.ckpt'\n",
    ")\n",
    "\n",
    "best_model.eval()  # enter prediction mode, e.g. turn off dropout\n",
    "\n",
    "print(best_model.data.data_split)  # confirm the data is not saved\n",
    "\n",
    "def test_and_print(te_dataset_object):\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset     = te_dataset_object,\n",
    "        batch_size  = best_model.hparams.batch_size,\n",
    "        collate_fn  = best_model.prepare_sample,\n",
    "        num_workers = best_model.hparams.loader_workers,\n",
    "    )\n",
    "    #print('number of batches:', len(test_dataloader))\n",
    "    new_trainer = pl.Trainer(\n",
    "        gpus = -1,\n",
    "        # https://github.com/PyTorchLightning/pytorch-lightning/issues/6690\n",
    "        logger = pl.loggers.TensorBoardLogger(os.path.abspath('lightning_logs')),\n",
    "    )\n",
    "    if best_model.tokenizer is None:\n",
    "        #print('setting tokeniser')\n",
    "        best_model.tokenizer = tokeniser\n",
    "    result = new_trainer.test(best_model, test_dataloaders = [test_dataloader])\n",
    "    assert len(result) == 1\n",
    "    return result[0]['test_acc']\n",
    "    \n",
    "def get_subset_by_domain(dataset_object, domain):\n",
    "    indices = []\n",
    "    for index in range(len(dataset_object)):\n",
    "        if domain == dataset_object[index]['domain']:\n",
    "            indices.append(index)\n",
    "    return torch.utils.data.Subset(dataset_object, indices)\n",
    "\n",
    "summary = []\n",
    "header = []\n",
    "header.append('Q')\n",
    "header.append('Overall')\n",
    "if len(domains) > 1:\n",
    "    for domain in sorted(list(domains)):\n",
    "        header.append(domain.title())\n",
    "header.append('Description')\n",
    "summary.append('\\t'.join(header))\n",
    "for te_index, te_dataset_object in enumerate(te_dataset_objects): \n",
    "    row = []\n",
    "    row.append('%d' %te_index)\n",
    "    print('\\nQuestion template %d: %r' %(te_index, templates[te_index]['question']))\n",
    "    score = 100.0 * test_and_print(te_dataset_object)\n",
    "    row.append('%.1f' %score)\n",
    "    if len(domains) > 1:\n",
    "        print('\\nBreakdown by domain:')\n",
    "        for domain in sorted(list(domains)):\n",
    "            print(domain)\n",
    "            score = 100.0 * test_and_print(\n",
    "                get_subset_by_domain(te_dataset_object, domain)\n",
    "            )\n",
    "            row.append('%.1f' %score)\n",
    "    print()\n",
    "    row.append(templates[te_index]['description'])\n",
    "    summary.append('\\t'.join(row))\n",
    "print('\\nSummary:')\n",
    "print('\\n'.join(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f85d9",
   "metadata": {},
   "source": [
    "## Appendix A: Example BERT Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1a3a5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tinput:         This computer is absolutely AMAZING!!!\n",
      "\t['input_ids']: [101, 2023, 3274, 2003, 7078, 6429, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'this', 'computer', 'is', 'absolutely', 'amazing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "1 \tinput:         and plenty of storage with 250 gb(though I will upgrade this and the ram..)\n",
      "\t['input_ids']: [101, 1998, 7564, 1997, 5527, 2007, 5539, 16351, 1006, 2295, 1045, 2097, 12200, 2023, 1998, 1996, 8223, 1012, 1012, 1007, 102]\n",
      "\ttokens:        ['[CLS]', 'and', 'plenty', 'of', 'storage', 'with', '250', 'gb', '(', 'though', 'i', 'will', 'upgrade', 'this', 'and', 'the', 'ram', '.', '.', ')', '[SEP]']\n",
      "\n",
      "2 \tinput:         GET THIS COMPUTER FOR PORTABILITY AND FAST PROCESSING!!!\n",
      "\t['input_ids']: [101, 2131, 2023, 3274, 2005, 3417, 8010, 1998, 3435, 6364, 999, 999, 999, 102]\n",
      "\ttokens:        ['[CLS]', 'get', 'this', 'computer', 'for', 'port', '##ability', 'and', 'fast', 'processing', '!', '!', '!', '[SEP]']\n",
      "\n",
      "3 \tinput:         without a big ol' clunky machine in my backpack, I feel like I can do programming homework anywhere.\n",
      "\t['input_ids']: [101, 2302, 1037, 2502, 19330, 1005, 18856, 16814, 2100, 3698, 1999, 2026, 13383, 1010, 1045, 2514, 2066, 1045, 2064, 2079, 4730, 19453, 5973, 1012, 102]\n",
      "\ttokens:        ['[CLS]', 'without', 'a', 'big', 'ol', \"'\", 'cl', '##unk', '##y', 'machine', 'in', 'my', 'backpack', ',', 'i', 'feel', 'like', 'i', 'can', 'do', 'programming', 'homework', 'anywhere', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "example_batch = []\n",
    "for i in (0, 4, 8, 18):  # select a few interesting instances\n",
    "    example_batch.append(tr_dataset[i][2])   \n",
    "\n",
    "tokenised_text = tokeniser(\n",
    "    example_batch,\n",
    "    is_split_into_words = False,\n",
    ")\n",
    "\n",
    "for i, token_ids in enumerate(tokenised_text['input_ids']):\n",
    "    if i: print()\n",
    "    print(i, '\\tinput:        ', example_batch[i])\n",
    "    print(   \"\\t['input_ids']:\", token_ids)\n",
    "    print(   '\\ttokens:       ', tokeniser.convert_ids_to_tokens(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee01347f",
   "metadata": {},
   "source": [
    "## Appendix B: Sequence Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d52b55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laptop\n",
      "LengthBin\t    negative\t     neutral\t    positive\t       Total\t  Positivity\n",
      "   0-   9\t         114\t          20\t         264\t         398\t        66%\n",
      "  10-  19\t         462\t          71\t         707\t        1240\t        57%\n",
      "  20-  29\t         286\t          57\t         411\t         754\t        55%\n",
      "  30-  39\t          99\t          15\t         115\t         229\t        50%\n",
      "  40-  49\t          46\t          13\t          34\t          93\t        37%\n",
      "  50-  59\t           8\t           2\t          20\t          30\t        67%\n",
      "  60-  69\t           8\t           0\t           0\t           8\t         0%\n",
      "  70-  79\t           3\t           0\t           2\t           5\t        40%\n",
      "  80-  89\t           3\t           0\t           0\t           3\t         0%\n",
      "\n",
      "restaurant\n",
      "LengthBin\t    negative\t     neutral\t    positive\t       Total\t  Positivity\n",
      "   0-   9\t          71\t          11\t         262\t         344\t        76%\n",
      "  10-  19\t         282\t          51\t         686\t        1019\t        67%\n",
      "  20-  29\t         175\t          20\t         409\t         604\t        68%\n",
      "  30-  39\t         108\t          10\t         140\t         258\t        54%\n",
      "  40-  49\t          38\t           3\t          39\t          80\t        49%\n",
      "  50-  59\t          25\t           0\t          26\t          51\t        51%\n",
      "  60-  69\t           9\t           0\t           6\t          15\t        40%\n",
      "  70-  79\t           3\t           0\t           0\t           3\t         0%\n",
      "  80-  89\t           0\t           0\t           0\t           0\t        n/a\n",
      "  90-  99\t           0\t           0\t           6\t           6\t       100%\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "    \n",
    "bin_width = 10\n",
    "\n",
    "for is_not_first, p_domain in enumerate(domains):\n",
    "    if is_not_first:\n",
    "        print()\n",
    "    print(p_domain)\n",
    "    distribution = defaultdict(lambda: 0)\n",
    "    tr_dataset.append(8*[None])  # hack to simplify loop below\n",
    "    batch = []\n",
    "    labels = []\n",
    "    max_length_bin = 0\n",
    "    for domain, _, text, _, _, _, _, label in tr_dataset:\n",
    "        if domain != p_domain:\n",
    "            continue\n",
    "        if text is not None:\n",
    "            batch.append(text)\n",
    "            labels.append(label)\n",
    "        if len(batch) == batch_size \\\n",
    "        or (text is None and len(batch) > 0):\n",
    "            tokenised_batch = tokeniser(\n",
    "                batch,\n",
    "                is_split_into_words = False,\n",
    "            )\n",
    "            for index, token_ids in enumerate(tokenised_batch['input_ids']):\n",
    "                label = labels[index]\n",
    "                length = len(token_ids)\n",
    "                length_bin = length // bin_width\n",
    "                distribution[(label,   length_bin)] += 1\n",
    "                distribution[('total', length_bin)] += 1\n",
    "                if length_bin > max_length_bin:\n",
    "                    max_length_bin = length_bin\n",
    "            batch = []\n",
    "            labels = []\n",
    "    del tr_dataset[-1]  # remove \"end of data\" marker of hack above   \n",
    "    header = []\n",
    "    header.append('LengthBin')\n",
    "    for polarity in sorted(tr_observed_polarities):\n",
    "        header.append('%12s' %polarity)\n",
    "    header.append('%12s' %'Total')\n",
    "    header.append('%12s' %'Positivity')\n",
    "    print('\\t'.join(header))\n",
    "    for length_bin in range(0, max_length_bin+1):\n",
    "        row = []\n",
    "        row.append('%4d-%4d' %(\n",
    "            bin_width*length_bin,\n",
    "            bin_width*(1+length_bin)-1\n",
    "        ))\n",
    "        total = 0\n",
    "        for label in sorted(tr_observed_polarities):\n",
    "            count = distribution[(label, length_bin)]\n",
    "            row.append('%12d' %count)\n",
    "            total += count\n",
    "        row.append('%12d' %total)\n",
    "        if total:\n",
    "            row.append('%10.0f%%' %(100.0*distribution[('positive', length_bin)]/float(total)))\n",
    "        else:\n",
    "            row.append('%11s' %'n/a')\n",
    "        print('\\t'.join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f75282",
   "metadata": {},
   "source": [
    "## Appendix C: Example Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba5b5d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'seq_A': 'laptop: Is there positive sentiment towards the GENERAL aspect of LAPTOP?', 'seq_B': 'This computer is absolutely AMAZING!!!', 'label': 'yes', 'domain': 'laptop', 'info': None}\n",
      "0 {'seq_A': 'laptop: Is there positive sentiment towards the GENERAL aspect of LAPTOP?', 'seq_B': 'This computer is absolutely AMAZING!!!', 'label': 'yes', 'domain': 'laptop', 'info': None}\n",
      "0 {'seq_A': 'laptop: Do you agree that the sentiment towards the aspect GENERAL of LAPTOP in the following review is negative?', 'seq_B': 'This computer is absolutely AMAZING!!!', 'label': 'no', 'domain': 'laptop', 'info': None}\n",
      "1 {'seq_A': 'laptop: The polarity of the aspect OPERATION_PERFORMANCE of BATTERY is neutral.', 'seq_B': '10 plus hours of battery...', 'label': 'no', 'domain': 'laptop', 'info': None}\n",
      "1 {'seq_A': 'laptop: Do you agree that the sentiment towards the aspect OPERATION_PERFORMANCE of BATTERY in the following review is negative?', 'seq_B': '10 plus hours of battery...', 'label': 'no', 'domain': 'laptop', 'info': None}\n",
      "1 {'seq_A': 'laptop: Is there neutral sentiment towards the OPERATION_PERFORMANCE aspect of BATTERY?', 'seq_B': '10 plus hours of battery...', 'label': 'no', 'domain': 'laptop', 'info': None}\n"
     ]
    }
   ],
   "source": [
    "dataset_obj = ABSA_Dataset(\n",
    "    tr_dataset,\n",
    "    put_question_first = put_question_first,\n",
    "    template_index = -1,   # -1 = random pick\n",
    ")\n",
    "for i in range(2):\n",
    "    print(i, dataset_obj[i])\n",
    "    print(i, dataset_obj[i])  # repeat call doesn't give the same result with template_index = -1\n",
    "    print(i, dataset_obj[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2462c39",
   "metadata": {},
   "source": [
    "## Appendix D: Location of Instances Picked for Devset\n",
    "Showing first 750 instances only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76188f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________d_____________________________________________________d\n",
      "_____________________________________d_________________d___________________\n",
      "d___________________________d___d___________d____d_________________________\n",
      "__dd_______________________________________________________________________\n",
      "__________d________________________________________________________________\n",
      "d________________________d__d_______________d______________________________\n",
      "______d_d_________________________________________________________d________\n",
      "___________d__________d_________________________________________________d__\n",
      "______________________________________________d__d_________________________\n",
      "________________________________________d____________________________d_____\n"
     ]
    }
   ],
   "source": [
    "row = []\n",
    "n = len(tr_indices) + len(dev_indices)\n",
    "for i in range(n):\n",
    "    if i in dev_indices:\n",
    "        row.append('d')\n",
    "    else:\n",
    "        row.append('_')\n",
    "    if len(row) == 75 or i+1 == n:\n",
    "        print(''.join(row))\n",
    "        row = []\n",
    "    if i > 750:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b987f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f691b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'question': '%(domain)s: The polarity of the aspect %(attribute_label)s of %(entity_type)s is %(candidate_polarity)s.', 'label': '%(yesno)s', 'description': 'Sun et al. QA-B'}, {'question': '%(domain)s: %(entity_type)s - %(attribute_label)s - %(candidate_polarity)s', 'label': '%(yesno)s', 'description': 'Sun et al. NLI-B'}, {'question': '%(domain)s: Do you agree that the sentiment towards the aspect %(attribute_label)s of %(entity_type)s in the following review is %(candidate_polarity)s?', 'label': '%(yesno)s', 'description': 'Variant 3, QA-B'}, {'question': '%(domain)s: Is there %(candidate_polarity)s sentiment towards the %(attribute_label)s aspect of %(entity_type)s?', 'label': '%(yesno)s', 'description': 'Variant 4, QA-B'}]\n"
     ]
    }
   ],
   "source": [
    "print(templates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca1e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
